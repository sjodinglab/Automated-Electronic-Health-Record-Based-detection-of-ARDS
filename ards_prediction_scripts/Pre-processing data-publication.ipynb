{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort big dataframe\n",
    "def sortdf(data,temp,colstosort):\n",
    "\n",
    "    asc=[True]*len(colstosort)\n",
    "    temp=temp.sort_values(colstosort, ascending=asc)\n",
    "\n",
    "    index=temp.index\n",
    "\n",
    "    data=data.reindex(index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the values that are still missing\n",
    "#get dummy variables\n",
    "def process(data):\n",
    "\n",
    "    structured_bin=data.copy()\n",
    "    structured_bin.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "    #if data is still missing, carry forward 1 bin\n",
    "    structured_bin=structured_bin.groupby(['EncounterID']).ffill(limit=1)\n",
    "\n",
    "    #Fill in the values that are still missing\n",
    "    structured_bin=structured_bin.groupby(['EncounterID','PatientID']).fillna(structured_bin.median()).reset_index(drop=False)\n",
    "\n",
    "    structured_bin=structured_bin.fillna('unknown')\n",
    "\n",
    "    #get dummy varibale\n",
    "    structured_bin=structured_bin.drop(['level_2'],1)\n",
    "    temp=structured_bin[['EncounterID','PatientID','ards','time']].copy()\n",
    "    structured_bin=pd.get_dummies(structured_bin.drop(['EncounterID','PatientID','ards','time'],1))\n",
    "    structured_bin['PatientID']=temp['PatientID'].copy()\n",
    "    structured_bin['ards']=temp['ards'].copy()\n",
    "    structured_bin['time']=temp['time'].copy()\n",
    "    structured_bin['EncounterID']=temp['EncounterID'].copy()\n",
    "\n",
    " \n",
    "\n",
    "    return structured_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_large(data1,data2,how,mergeon):\n",
    "    encounterids=data1.EncounterID.unique().tolist()\n",
    "    size=100\n",
    "    list_of_encounter = [encounterids[i:i+size] for i in range(0, len(encounterids),size)]\n",
    "\n",
    "    k=0\n",
    "    for e in list_of_encounter:\n",
    "        #print(k)\n",
    "        k+=1\n",
    "        temp=pd.merge(data1[data1.EncounterID.isin(e)],data2[data2.EncounterID.isin(e)],how=how,on=mergeon)\n",
    "        temp=sortdf(temp,temp[['EncounterID','time']],['EncounterID','time'])\n",
    "        if k==1:\n",
    "            temp.to_csv(PATH5+'temp.csv',index=False,header='column_names')\n",
    "        else:\n",
    "            temp.to_csv(PATH5+'temp.csv', mode='a', header=None,index=False)\n",
    "        del temp\n",
    "    \n",
    "    merged=pd.read_csv(PATH5+'temp.csv')\n",
    "    merged.time=pd.to_datetime(merged.time)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carryhours(data,col,hours):\n",
    "    newcol=col+'carry'\n",
    "    data.loc[pd.notnull(data[col]),newcol]=data['time']\n",
    "    data[newcol]=data.groupby(['EncounterID'])[newcol].ffill()\n",
    "    data[col]=data.groupby(['EncounterID'])[col].ffill()\n",
    "    data.loc[(data['time']-data[newcol])>timedelta(hours=hours),col]=np.nan\n",
    "    data=data.drop(newcol,1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(include_othernotes_merge,include_radiology50,bintest):\n",
    "    #read in data\n",
    "    structured=pd.read_csv(PATH2+'structured_data.csv')\n",
    "    structured_features=pd.read_csv(PATH2+'publication_structured_features_list.csv')\n",
    "    feature_list=structured_features.features.tolist() #list of structured data features;\n",
    "    structured=structured[feature_list]\n",
    "    pid=structured[['EncounterID','PatientID']].drop_duplicates(keep='first')\n",
    "    structured.time=pd.to_datetime(structured.time)\n",
    "    \n",
    "    \n",
    "    structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "    \n",
    "    #calculate percentage of missing value\n",
    "\n",
    "    structured['null_percent']=structured.isnull().sum(axis=1)\n",
    "\n",
    "    structured['null_percent']=structured['null_percent']/(len(structured.columns)-3)\n",
    "    \n",
    "    #carry forward support\n",
    "    structured['support']=structured.groupby(['EncounterID'])['support'].ffill()\n",
    "\n",
    "    #get structured data's column names\n",
    "    structured_cols=structured.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "\n",
    "    if include_radiology50:\n",
    "        #merge with the unstructured data\n",
    "        unstructured=pd.read_csv(PATH2+'radiology_report_ctakes_features.csv')\n",
    "        unstructured.time=pd.to_datetime(unstructured.time)\n",
    "\n",
    "        #add PatientID\n",
    "        unstructured=pd.merge(unstructured,pid,how='left',on='EncounterID')\n",
    "\n",
    "        structured=pd.merge(structured,unstructured,how='outer',on=['EncounterID','PatientID','time'])\n",
    "        #sort\n",
    "        structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "\n",
    "        #carry forward unstructured data within each encounter\n",
    "        unstructuredcols=unstructured.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "        #rename\n",
    "        for col in unstructuredcols:\n",
    "            tempcol=col+'xray'\n",
    "            structured.rename(columns={col: tempcol},inplace=True)\n",
    "        \n",
    "        for col in unstructuredcols:\n",
    "            tempcol=col+'xray'\n",
    "            structured[tempcol]=structured.groupby(['EncounterID'])[tempcol].ffill()\n",
    "\n",
    "        #drop rows that only have unstructured data\n",
    "        structured = structured.dropna(subset=structured_cols, how='all')\n",
    "\n",
    "        unstructuredcols=[i+'xray' for i in unstructuredcols]\n",
    "        #fill in 0 when the unstructured data is missing\n",
    "        structured[unstructuredcols] = structured[unstructuredcols].fillna(value=0)\n",
    "\n",
    "    #merge with clinical notes' ctakes features \n",
    "    if include_othernotes_merge:\n",
    "\n",
    "        othernotes=pd.read_csv(PATH2+'clinical_notes_ctakes_features.csv')\n",
    "        othernotes.time=pd.to_datetime(othernotes.time)\n",
    "        othernotes=pd.merge(othernotes,pid,how='left',on='EncounterID')\n",
    "        othercols=othernotes.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "\n",
    "        encounterids=structured.EncounterID.unique()\n",
    "        size=100\n",
    "        list_of_encounter = [encounterids[i:i+size] for i in range(0, len(encounterids),size)]\n",
    "\n",
    "        k=0\n",
    "        for e in list_of_encounter:\n",
    "            #print(k)\n",
    "            k+=1\n",
    "            temp=pd.merge(structured[structured.EncounterID.isin(e)],othernotes[othernotes.EncounterID.isin(e)],how='outer',on=['EncounterID','PatientID','time'])\n",
    "            temp=sortdf(temp,temp[['EncounterID','time']],['EncounterID','time'])\n",
    "            for col in othercols:\n",
    "                temp[col]=temp.groupby(['EncounterID'])[col].ffill()\n",
    "           \n",
    "            temp = temp.dropna(subset=structured_cols, how='all')\n",
    "            temp[othercols]=temp[othercols].fillna(0)\n",
    "            if k==1:\n",
    "                temp.to_csv(PATH5+'merge_clinical.csv',index=False,header='column_names')\n",
    "            else:\n",
    "                temp.to_csv(PATH5+'merge_clinical.csv', mode='a', header=None,index=False)\n",
    "            del temp\n",
    "\n",
    "        del othernotes\n",
    "        del structured\n",
    "\n",
    "        structured=pd.read_csv(PATH5+'merge_clinical.csv')\n",
    "        structured.time=pd.to_datetime(structured.time)       \n",
    "        \n",
    "    if include_othernotes_merge and include_radiology50:\n",
    "        #merge two columns if the cui code is the same\n",
    "        allcols=structured.columns.tolist()\n",
    "        \n",
    "        notecols=[col for col in allcols if col[1:].isdecimal()]\n",
    "        \n",
    "        for col in notecols:\n",
    "            tempcol=col+'xray'\n",
    "            if tempcol in allcols:\n",
    "                structured.loc[structured[tempcol]==1,col]=1\n",
    "                structured=structured.drop(tempcol,1)\n",
    "                \n",
    "\n",
    "    structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "    \n",
    "    #deal with missing height and weight\n",
    "    #each encounter was assigned with 1 height and 1 weight\n",
    "    height=structured.groupby(['EncounterID','PatientID'])['height'].mean().reset_index(drop=False)\n",
    "\n",
    "    weight=structured.groupby(['EncounterID','PatientID'])['weight'].mean().reset_index(drop=False)\n",
    "    \n",
    "    age=structured.groupby(['EncounterID','PatientID'])['AgeInYears'].first().reset_index(drop=False)\n",
    "\n",
    "    gender=structured.groupby(['EncounterID','PatientID'])['GenderCode'].first().reset_index(drop=False)\n",
    "\n",
    "\n",
    "    structured=structured.drop(['height','weight','AgeInYears','GenderCode'],1)\n",
    "\n",
    "    structured=pd.merge(structured,height,how='left',on=['EncounterID','PatientID'])\n",
    "    structured=pd.merge(structured,weight,how='left',on=['EncounterID','PatientID'])\n",
    "    structured=pd.merge(structured,age,how='left',on=['EncounterID','PatientID'])\n",
    "    structured=pd.merge(structured,gender,how='left',on=['EncounterID','PatientID'])\n",
    "    \n",
    "    \n",
    "    ####deal with missing data\n",
    "    structured.time=pd.to_datetime(structured.time)\n",
    "    #carry forward\n",
    "    carrydic={'temp':'8H','hr':'8H','rr':'8H','sbp':'8H','dbp':'8H','gcs':'24H','rass':'24H','shock_indx':'8H',\n",
    "\n",
    "             'spo2':'8H','fio2':'8H','pf':'48H','sf':'48H','support':'encounter',\n",
    "\n",
    "             'peep':'support','plat':'support','mairp':'support','ve':'support',\n",
    "\n",
    "            'o2flow_rate':'support', 'Vte':'support', 'Vtset':'support',\n",
    "\n",
    "            'Compliance':'support', 'VR':'support', 'oi':'support',\n",
    "\n",
    "            'lactate':'48H','ph':'48H','paco2':'48H','pao2': '48H',\n",
    "\n",
    "            'na': 'encounter','k': 'encounter','hco2': 'encounter','bun': 'encounter','cr':'encounter',\n",
    "\n",
    "             'alb':'encounter','tp':'encounter','tbili':'encounter','ast':'encounter','hgb': 'encounter','wbc':'encounter',\n",
    "\n",
    "             'plt': 'encounter','inr': 'encounter','ptt': 'encounter','bnp':'encounter','trop':'encounter','procalcitonin':'encounter',\n",
    "\n",
    "             'd-dimer':'encounter'}\n",
    "\n",
    "    for col in carrydic.keys():\n",
    "        #carry forward if the encounterid didn't change\n",
    "        if carrydic[col]=='encounter':\n",
    "            structured[col]=structured.groupby(['EncounterID'])[col].ffill()\n",
    "        #carry forward if the support type didn't change\n",
    "        elif carrydic[col]=='support':\n",
    "            structured[col]=structured.groupby(['EncounterID','support'])[col].ffill()\n",
    "        #carry forward N hours\n",
    "        else:\n",
    "            structured=carryhours(structured,col,int(carrydic[col][:-1]))\n",
    "            #structured[col]=structured.groupby(['EncounterID',pd.Grouper(key='time', freq=carrydic[col])])[col].ffill()\n",
    "\n",
    "\n",
    "    ###bining the data \n",
    "\n",
    "    aggregation_functions = {}\n",
    "    floatcols=structured.loc[:, structured.dtypes == np.float64].columns.tolist()\n",
    "\n",
    "    \n",
    "    minmaxcols=['plat','mairp','pf','sf','oi','peep']\n",
    "    maxcols=['temp' ,'hr', 'rr', 'shock_indx' ,'fio2','o2flow_rate', 'bnp','procalcitonin','inr','fluid_bal','ve','paco2','VR','ptt','lactate','paco2','wbc', 'ddimer','trop']\n",
    "    mincols=['dbp','sbp','gcs','rass', 'spo2','alb','plt', 'tp','hgb','Vtset', 'pao2','Compliance','ph', 'na','k','hco2', 'ast']\n",
    "    meancols=['ards_scale_1','ra','null_percent','height','weight']\n",
    "    #set the aggregation function for binning the data\n",
    "    for col in structured.columns:\n",
    "        if col=='ards' :\n",
    "            aggregation_functions[col]='max'\n",
    "        elif include_orders and col in orderscols :\n",
    "            aggregation_functions[col]='max'\n",
    "        elif include_othernotes_merge and col in othercols:\n",
    "            aggregation_functions[col]='max'\n",
    "        elif (include_unstructured or include_radiology50) and col in unstructuredcols:\n",
    "            aggregation_functions[col]='max'\n",
    "        elif col in meancols:\n",
    "            aggregation_functions[col]='mean'\n",
    "        elif col in ['EncounterID','PatientID','time']:\n",
    "            continue\n",
    "        elif col in minmaxcols:\n",
    "            name=col+'_min'\n",
    "            aggregation_functions[name]='min'\n",
    "            name=col+'_max'\n",
    "            aggregation_functions[name]='max'\n",
    "        elif col in maxcols:\n",
    "            aggregation_functions[col]='max'\n",
    "        elif col in mincols:\n",
    "            aggregation_functions[col]='min'\n",
    "        elif col in floatcols:   \n",
    "            aggregation_functions[col]='min'\n",
    "        else:\n",
    "            aggregation_functions[col]='last'\n",
    "            \n",
    "    \n",
    "    for col in minmaxcols:\n",
    "        name=col+'_min'\n",
    "        structured[name]=structured[col].copy()\n",
    "        name=col+'_max'\n",
    "        structured[name]=structured[col].copy()\n",
    "        structured=structured.drop(col,1)\n",
    "    \n",
    "    print(aggregation_functions)\n",
    "\n",
    "    \n",
    "    structured_bin2 = structured.groupby(['EncounterID','PatientID',pd.Grouper(key='time', freq=(str(bintest)+'H'))]).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "\n",
    "    #fill in missing values with predefined values\n",
    "    fillin={'temp':98.3,'hr':80,'rr':20,'sbp':110,'dbp':60,'gcs':15,'rass':0,'shock_indx':0.7,\n",
    "            'spo2':98, 'fio2':21,'pf':400,'sf':400,'peep':0,'plat':5,'mairp':5,'ve':5,\n",
    "            'o2flow_rate':0,'Vte':400,'Vtset':400,'oi':1, 'Compliance':50,'VR':1,\n",
    "            'lactate':0,'ph':7.4,'paco2':40,'pao2':90,'na':140,\n",
    "            'k':4,'hco2':24,'bun':25,'hgb':12,'wbc':10,'plt':150,'inr':1,'ptt':1}\n",
    "    \n",
    "    for col in fillin.keys():\n",
    "        if col in minmaxcols:\n",
    "            name=col+'_min'\n",
    "            structured_bin2[name]=structured_bin2[name].fillna(fillin[col])\n",
    "            name=col+'_max'\n",
    "            structured_bin2[name]=structured_bin2[name].fillna(fillin[col])\n",
    "        else:\n",
    "            structured_bin2[col]=structured_bin2[col].fillna(fillin[col])\n",
    "\n",
    "    test_str=process(structured_bin2)\n",
    "    \n",
    "    filename='structured'\n",
    "    if include_radiology50:\n",
    "        filename+='+radiology50'\n",
    "    if include_othernotes_merge:\n",
    "        filename+='+clinical_notes250'\n",
    "\n",
    "    features=pd.read_csv(PATH2+filename+'_final_features_list.csv')\n",
    "    feature_list=features.features.tolist() \n",
    "    test_str=test_str[final_features.columns] \n",
    "\n",
    "    test_str.to_csv(PATH5+filename+'_'+str(bintest)+'Htest.csv',index=False)\n",
    "    print(filename+'_'+str(bintest)+'Htest.csv')\n",
    "    \n",
    "    return 'finished'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of the directory that stores the input data: \n",
    "#~/input_files/structured_data.csv\n",
    "#~/input_files/publication_structured_features_list.csv\n",
    "#~/input_files/radiology_report_ctakes_features.csv\n",
    "#~/input_files/clinical_notes_ctakes_features.csv\n",
    "#~/input_files/structured_final_features_list.csv\n",
    "#~/input_files/structured+radiology50+clinical_notes250_final_features_list.csv\n",
    "PATH2='' \n",
    "PATH5=''#path of the directory that stores the output datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[ #structured\n",
    "             {'include_othernotes_merge':False,'bintest':2,'include_radiology50':False}\n",
    "            #structured+radiology50+clinicalnotes250\n",
    "             {'include_othernotes_merge':True,'bintest':2,'include_radiology50':True}\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    print('***********',p)\n",
    "    preprocessing(**p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
