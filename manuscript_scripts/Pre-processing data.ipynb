{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there is more than two columns to the data, you can just read in\n",
    "# the two columns of data, try to sort it, and then save the index of the\n",
    "# sorted dataframe. Then you can read in the original data and rearrange it\n",
    "# using the sorted index.\n",
    "def sortdf(data,temp,colstosort):\n",
    "\n",
    "    asc=[True]*len(colstosort)\n",
    "    temp=temp.sort_values(colstosort, ascending=asc)\n",
    "\n",
    "    index=temp.index\n",
    "\n",
    "    data=data.reindex(index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in the values that are still missing\n",
    "#get dummy variables\n",
    "#convert ards_scale_1 to sample weight\n",
    "def process(data,trainset,testset):\n",
    "\n",
    "    structured_bin=data.copy()\n",
    "    structured_bin.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "\n",
    "    #if data is still missing, carry forward 1 bin\n",
    "    structured_bin=structured_bin.groupby(['EncounterID']).ffill(limit=1)\n",
    "\n",
    "    #Fill in the values that are still missing\n",
    "    structured_bin=structured_bin.groupby(['EncounterID','PatientID']).fillna(structured_bin.median()).reset_index(drop=False)\n",
    "\n",
    "    structured_bin=structured_bin.fillna('unknown')\n",
    "\n",
    "\n",
    "    #convert ards_scale_1 to sample weight\n",
    "    structured_bin.loc[(structured_bin['ards_scale_1']<=1) & (structured_bin['ards_scale_1']>=0.5),'sampleweight']=structured_bin['ards_scale_1']\n",
    "    structured_bin.loc[(structured_bin['ards_scale_1']<0.5) ,'sampleweight']=1-structured_bin['ards_scale_1']\n",
    "    structured_bin=structured_bin.drop(['ards_scale_1'],1)\n",
    "\n",
    "    #get dummy varibale\n",
    "    structured_bin=structured_bin.drop(['level_2'],1)\n",
    "    temp=structured_bin[['EncounterID','PatientID','ards','time']].copy()\n",
    "    structured_bin=pd.get_dummies(structured_bin.drop(['EncounterID','PatientID','ards','time'],1))\n",
    "    structured_bin['PatientID']=temp['PatientID'].copy()\n",
    "    structured_bin['ards']=temp['ards'].copy()\n",
    "    structured_bin['time']=temp['time'].copy()\n",
    "    structured_bin['EncounterID']=temp['EncounterID'].copy()\n",
    "\n",
    "\n",
    "    #train test split\n",
    "    test_str=structured_bin[structured_bin['EncounterID'].isin(testset)]\n",
    "    train_str=structured_bin[structured_bin['EncounterID'].isin(trainset)]\n",
    "\n",
    " \n",
    "\n",
    "    return train_str,test_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_large(data1,data2,how,mergeon):\n",
    "    encounterids=data1.EncounterID.unique().tolist()\n",
    "    size=100\n",
    "    list_of_encounter = [encounterids[i:i+size] for i in range(0, len(encounterids),size)]\n",
    "\n",
    "    k=0\n",
    "    for e in list_of_encounter:\n",
    "        #print(k)\n",
    "        k+=1\n",
    "        temp=pd.merge(data1[data1.EncounterID.isin(e)],data2[data2.EncounterID.isin(e)],how=how,on=mergeon)\n",
    "        temp=sortdf(temp,temp[['EncounterID','time']],['EncounterID','time'])\n",
    "        if k==1:\n",
    "            temp.to_csv(PATH5+'temp.csv',index=False,header='column_names')\n",
    "        else:\n",
    "            temp.to_csv(PATH5+'temp.csv', mode='a', header=None,index=False)\n",
    "        del temp\n",
    "    \n",
    "    merged=pd.read_csv(PATH5+'temp.csv')\n",
    "    merged.time=pd.to_datetime(merged.time)\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def carryhours(data,col,hours):\n",
    "    newcol=col+'carry'\n",
    "    data.loc[pd.notnull(data[col]),newcol]=data['time']\n",
    "    data[newcol]=data.groupby(['EncounterID'])[newcol].ffill()\n",
    "    data[col]=data.groupby(['EncounterID'])[col].ffill()\n",
    "    data.loc[(data['time']-data[newcol])>timedelta(hours=hours),col]=np.nan\n",
    "    data=data.drop(newcol,1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(include_unstructured,include_othernotes_merge,include_orders,clinicalnotes_only,ctakes_only,unigrambigram_only,unstructured_only,medication_only,include_radiology50,bintrain,bintest):\n",
    "    #read in data\n",
    "\n",
    "    ards=pd.read_csv(PATH1+'current-ards-review-results_2_25_2020.csv',dtype={'mrn': str})\n",
    "    ards.rename(columns={'encounterid':'EncounterID'},inplace=True)\n",
    "    ards.loc[ards['admitdate']=='.','admitdate']=np.nan\n",
    "    ards.admitdate=pd.to_datetime(ards.admitdate)\n",
    "\n",
    "    structured=pd.read_csv(PATH2+'structured_data.csv')\n",
    "    pid=structured[['EncounterID','PatientID']].drop_duplicates(keep='first')\n",
    "    structured.time=pd.to_datetime(structured.time)\n",
    "    structured=structured.drop(['fio2_carry','mairp_carry','invasive', 'noninvasive', 'supl','hfnc','ra',\n",
    "                               'vent_start', 'msc_rt',\n",
    "                                'niv_mode', 'niv_start'],1) \n",
    "    \n",
    "    structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "    \n",
    "    #calculate percentage of missing value\n",
    "\n",
    "    structured['null_percent']=structured.isnull().sum(axis=1)\n",
    "\n",
    "    structured['null_percent']=structured['null_percent']/(len(structured.columns)-3)\n",
    "    \n",
    "    #categorical variable mapping classes\n",
    "    #carry forward vend_mode\n",
    "    structured['vent_mode']=structured.groupby(['EncounterID','support'])['vent_mode'].ffill()\n",
    "    structured['resp_support']=structured.groupby(['EncounterID','support'])['resp_support'].ffill()\n",
    "    mapping=pd.read_csv(PATH2+'resp_map.csv')\n",
    "\n",
    "    for index,row in mapping.iterrows():\n",
    "        row['old']=row['old'].split('_')[2]\n",
    "        if row['old']!='unknown':\n",
    "            structured.loc[structured['resp_support']==row['old'],'resp_support']=row['new']\n",
    "        else:\n",
    "            structured.loc[structured['resp_support']==row['old'],'resp_support']='unknown'\n",
    "        \n",
    "    #combine support and resp_support\n",
    "    structured.loc[(structured['resp_support']=='nc') & (structured['support']=='supl'),'support']='supl_nc'\n",
    "    structured.loc[(structured['resp_support']=='mask')& (structured['support']=='supl'),'support']='supl_mask'\n",
    "    structured.loc[(structured['resp_support']=='nrb')& (structured['support']=='supl'),'support']='supl_nrb'\n",
    "    structured.loc[(structured['resp_support']=='trach')& (structured['support']=='supl'),'support']='supl_trach'\n",
    "    structured.loc[(structured['resp_support']=='ra') & (structured['support']=='supl'),'support']='ra'\n",
    "    structured.loc[(structured['resp_support']=='hhfnc') & (structured['support']=='supl'),'support']='hfnc'\n",
    "    structured.loc[(structured['resp_support']=='unknown')& (structured['support']=='supl'),'support']='unknown'\n",
    "    structured.loc[(structured['support']=='supl'),'support']='unknown'\n",
    "    \n",
    "    structured=structured.drop('resp_support',1)\n",
    "    \n",
    "    structured['support']=structured.groupby(['EncounterID'])['support'].ffill()\n",
    "\n",
    "\n",
    "    #get structured data's column names\n",
    "    structured_cols=structured.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "\n",
    "\n",
    "    if clinicalnotes_only or ctakes_only or unigrambigram_only or unstructured_only or medication_only:\n",
    "        if unstructured_only:\n",
    "            data=pd.read_csv(PATH2+'unstructured_reports_no_unigram_updated.csv')\n",
    "            data=data[['EncounterID','time', 'azzam_neg',\n",
    "               'herasevich_neg','ards_neg', 'edema_neg', 'infiltrate_neg',\n",
    "               'pneumonia_neg', 'airspace_neg', 'aspiration_neg', 'opacity_neg']]\n",
    "            \n",
    "        elif ctakes_only:\n",
    "            data=pd.read_csv(PATH2+'chest_xray_ctakes_1000_split1.csv')\n",
    "        \n",
    "        elif unigrambigram_only:\n",
    "            data=pd.read_csv(PATH2+'unigram+bigram.csv')\n",
    "        \n",
    "        elif clinicalnotes_only:\n",
    "            data=pd.read_csv(PATH2+'clinical_notes_ctakes_1000_split1.csv') \n",
    "            \n",
    "        elif medication_only:\n",
    "            data=pd.read_csv(PATH2+'treatments_orders_split1.csv')\n",
    "        \n",
    "        data.time=pd.to_datetime(data.time)\n",
    "        \n",
    "        data=merge_large(data,pid,'left','EncounterID')\n",
    "\n",
    "        if not medication_only:\n",
    "            #merge with pf\n",
    "            pf=structured[['EncounterID','time','pf']]\n",
    "            data=merge_large(data,pf,'outer',['EncounterID','time'])\n",
    "            #data=pd.merge(data,pf,how='outer',on=['EncounterID','time'])\n",
    "            cols=data.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "            #carry forward 24h\n",
    "            k=0\n",
    "            for col in cols:\n",
    "                #print(k)\n",
    "                k+=1\n",
    "                data=carryhours(data,col,24)\n",
    "                #data[col]=data.groupby(['EncounterID',pd.Grouper(key='time', freq='24H')])[col].ffill()\n",
    "\n",
    "            data=data.dropna(how='any')\n",
    "            \n",
    "            #print(data.shape)\n",
    "        \n",
    "        #merge with ards_scale\n",
    "        data=merge_large(data,ards[['EncounterID','ards_scale_1']],'left','EncounterID')\n",
    "        #data=pd.merge(data,ards[['EncounterID','ards_scale_1']],how='left',on='EncounterID')\n",
    "        \n",
    "        #merge with support\n",
    "        data=merge_large(data,structured[['EncounterID','time','support']],'outer',['EncounterID','time'])\n",
    "        cols=data.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "        #carry forward \n",
    "        k=0\n",
    "        for col in cols:\n",
    "            #print(k)\n",
    "            k+=1\n",
    "            data[col]=data.groupby(['EncounterID'])[col].ffill()\n",
    "\n",
    "        data=data.dropna(how='any')\n",
    "\n",
    "        #add labels\n",
    "        #replace missing value with nan\n",
    "        ards.loc[ards['ards_time']=='.','ards_time']=np.nan\n",
    "        ards.ards_time=pd.to_datetime(ards.ards_time)\n",
    "\n",
    "        #merge with ards_time\n",
    "        data=merge_large(data,ards[['EncounterID','ards_time']],'left','EncounterID')\n",
    "        data.ards_time=pd.to_datetime(data.ards_time)\n",
    "        #data=pd.merge(data,ards[['EncounterID','ards_time']],how='left',on='EncounterID')\n",
    "\n",
    "        #if there is no ards_time, the patient had never had ards so the label will be 0\n",
    "        data.loc[pd.isnull(data['ards_time']),'ards']=0\n",
    "        #if the patient had ards,records after ards_time labeled as 1\n",
    "        data.loc[(data['ards_time']-timedelta(hours=0))<=data['time'] ,'ards']=1\n",
    "        #if the patient had ards,records before ards_time labeled as 1\n",
    "        data.loc[pd.isnull(data['ards']),'ards']=0\n",
    "\n",
    "        data=data.drop(['ards_time'],1)\n",
    "        \n",
    "        #set ards_scale_1\n",
    "        #if pt_ards==0\n",
    "        data.loc[(data['EncounterID'].isin(ards[ards['pt_ards']==0].EncounterID.unique()))&(~data['support'].isin(['invasive','noninvasive','hfnc'])),'ards_scale_1']=1\n",
    "\n",
    "        #if pt_ards==1\n",
    "        data.loc[(data['EncounterID'].isin(ards[ards['pt_ards']==1].EncounterID.unique()))&(~data['support'].isin(['invasive','noninvasive','hfnc']))&(data['ards']==0),'ards_scale_1']=1\n",
    "        data.loc[(data['EncounterID'].isin(ards[ards['pt_ards']==1].EncounterID.unique()))&(data['support'].isin(['invasive','noninvasive','hfnc']))&(data['ards']==0),'ards_scale_1']=4\n",
    "        data.loc[(data['EncounterID'].isin(ards[ards['pt_ards']==1].EncounterID.unique()))&(~data['support'].isin(['invasive','noninvasive','hfnc']))&(data['ards']==1),'ards_scale_1']=4\n",
    "\n",
    "        data=data.drop(['support'],1)\n",
    "        #normalize the ards_scale_1\n",
    "        data['ards_scale_1']=(data['ards_scale_1']-min(data['ards_scale_1']))/(max(data['ards_scale_1'])-min(data['ards_scale_1']))   \n",
    "        \n",
    "        #train test split\n",
    "        trainset=ards[(ards['year']==2016)&(ards['not_reviewed']==0)&(ards['not_cohort']==0)&(pd.notnull(ards['pt_ards']))].EncounterID.unique().tolist()\n",
    "        testset=ards[(ards['year']==2017)&(ards['not_reviewed']==0)&(ards['not_cohort']==0)&(pd.notnull(ards['pt_ards']))].EncounterID.unique().tolist()\n",
    "\n",
    "        train_str,test_str=process(data,trainset,testset)\n",
    "      \n",
    "        if len(train_str.columns)>=len(test_str.columns):\n",
    "            train_str=train_str[test_str.columns]\n",
    "        else:\n",
    "            test_str=test_str[train_str.columns]\n",
    "\n",
    "        test_str=test_str[train_str.columns] \n",
    "        \n",
    "        if clinicalnotes_only:\n",
    "            filename='clinicalnotes_only1000'\n",
    "        if ctakes_only:\n",
    "            filename='ctakes_only_1000'\n",
    "        if unigrambigram_only:\n",
    "            filename='unigrambigram_only'\n",
    "        if unstructured_only:\n",
    "            filename='unstructured_only'\n",
    "        if medication_only:\n",
    "            filename='medication_only'\n",
    "\n",
    "        train_str.to_csv(PATH5+filename+'_'+'train.csv',index=False)\n",
    "        test_str.to_csv(PATH5+filename+'_'+'test.csv',index=False)\n",
    "        print(filename+'_'+'train.csv')\n",
    "        print(filename+'_'+'test.csv')\n",
    "        \n",
    "        return 'finished'\n",
    "    \n",
    "\n",
    "    if include_unstructured:\n",
    "        #merge with the unstructured data\n",
    "        unstructured=pd.read_csv(PATH2+'unstructured_reports_no_unigram_updated.csv')\n",
    "        unstructured.time=pd.to_datetime(unstructured.time)\n",
    "        unstructured=unstructured[['EncounterID','time', 'azzam_neg',\n",
    "               'herasevich_neg','ards_neg', 'edema_neg', 'infiltrate_neg',\n",
    "               'pneumonia_neg', 'airspace_neg', 'aspiration_neg', 'opacity_neg']]\n",
    "\n",
    "        #add PatientID\n",
    "        unstructured=pd.merge(unstructured,pid,how='left',on='EncounterID')\n",
    "\n",
    "        structured=pd.merge(structured,unstructured,how='outer',on=['EncounterID','PatientID','time'])\n",
    "        #sort\n",
    "        structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "\n",
    "        #carry forward unstructured data within each encounter\n",
    "        unstructuredcols=unstructured.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "        for col in unstructuredcols:\n",
    "            structured[col]=structured.groupby(['EncounterID'])[col].ffill()\n",
    "\n",
    "        #drop rows that only have unstructured data\n",
    "        structured = structured.dropna(subset=structured_cols, how='all')\n",
    "\n",
    "        #fill in 0 when the unstructured data is missing\n",
    "        structured[unstructuredcols] = structured[unstructuredcols].fillna(value=0)\n",
    "        \n",
    "        \n",
    "    if include_radiology50:\n",
    "        #merge with the unstructured data\n",
    "        unstructured=pd.read_csv(PATH2+'chest_xray_ctakes_50_split1.csv')\n",
    "        unstructured.time=pd.to_datetime(unstructured.time)\n",
    "\n",
    "        #add PatientID\n",
    "        unstructured=pd.merge(unstructured,pid,how='left',on='EncounterID')\n",
    "\n",
    "        structured=pd.merge(structured,unstructured,how='outer',on=['EncounterID','PatientID','time'])\n",
    "        #sort\n",
    "        structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "\n",
    "        #carry forward unstructured data within each encounter\n",
    "        unstructuredcols=unstructured.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "        #rename\n",
    "        for col in unstructuredcols:\n",
    "            tempcol=col+'xray'\n",
    "            structured.rename(columns={col: tempcol},inplace=True)\n",
    "        \n",
    "        for col in unstructuredcols:\n",
    "            tempcol=col+'xray'\n",
    "            structured[tempcol]=structured.groupby(['EncounterID'])[tempcol].ffill()\n",
    "\n",
    "        #drop rows that only have unstructured data\n",
    "        structured = structured.dropna(subset=structured_cols, how='all')\n",
    "\n",
    "        unstructuredcols=[i+'xray' for i in unstructuredcols]\n",
    "        #fill in 0 when the unstructured data is missing\n",
    "        structured[unstructuredcols] = structured[unstructuredcols].fillna(value=0)\n",
    "\n",
    "    if include_orders:\n",
    "        #merge with medication orders\n",
    "        orders=pd.read_csv(PATH2+'treatments_orders_split1.csv')\n",
    "        orders.time=pd.to_datetime(orders.time)\n",
    "\n",
    "        #add patient ID\n",
    "        orders=pd.merge(orders,pid,how='left',on='EncounterID')\n",
    "        structured=pd.merge(structured,orders,how='outer',on=['EncounterID','PatientID','time'])\n",
    "\n",
    "        structured=structured.sort_values(['EncounterID','time'], ascending=[True,True])\n",
    "\n",
    "        #carry forward 72 h\n",
    "        orderscols=orders.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "        for col in orderscols:\n",
    "            structured=carryhours(structured,col,72)\n",
    "            structured[col]=structured[col].fillna(0)\n",
    "\n",
    "        #drop rows that only have orders data\n",
    "        structured = structured.dropna(subset=structured_cols, how='all')\n",
    "            \n",
    "\n",
    "    #merge with clinical notes' ctakes features \n",
    "    if include_othernotes_merge:\n",
    "\n",
    "        othernotes=pd.read_csv(PATH2+'clinical_notes_ctakes_250_split1.csv')\n",
    "        othernotes.time=pd.to_datetime(othernotes.time)\n",
    "        othernotes=pd.merge(othernotes,pid,how='left',on='EncounterID')\n",
    "        othercols=othernotes.drop(['EncounterID','PatientID','time'],1).columns.tolist()\n",
    "\n",
    "        encounterids=structured.EncounterID.unique()\n",
    "        size=100\n",
    "        list_of_encounter = [encounterids[i:i+size] for i in range(0, len(encounterids),size)]\n",
    "\n",
    "        #print(len(list_of_encounter))\n",
    "        k=0\n",
    "        for e in list_of_encounter:\n",
    "            #print(k)\n",
    "            k+=1\n",
    "            temp=pd.merge(structured[structured.EncounterID.isin(e)],othernotes[othernotes.EncounterID.isin(e)],how='outer',on=['EncounterID','PatientID','time'])\n",
    "            temp=sortdf(temp,temp[['EncounterID','time']],['EncounterID','time'])\n",
    "            for col in othercols:\n",
    "                temp[col]=temp.groupby(['EncounterID'])[col].ffill()\n",
    "           \n",
    "            temp = temp.dropna(subset=structured_cols, how='all')\n",
    "            temp[othercols]=temp[othercols].fillna(0)\n",
    "            if k==1:\n",
    "                temp.to_csv(PATH5+'merge_clinical.csv',index=False,header='column_names')\n",
    "            else:\n",
    "                temp.to_csv(PATH5+'merge_clinical.csv', mode='a', header=None,index=False)\n",
    "            del temp\n",
    "\n",
    "        del othernotes\n",
    "        del structured\n",
    "\n",
    "        structured=pd.read_csv(PATH5+'merge_clinical.csv')\n",
    "        structured.time=pd.to_datetime(structured.time)       \n",
    "        \n",
    "    if include_othernotes_merge and include_radiology50:\n",
    "        #merge two columns if the cui code is the same\n",
    "        allcols=structured.columns.tolist()\n",
    "        \n",
    "        notecols=[col for col in allcols if col[1:].isdecimal()]\n",
    "        \n",
    "        for col in notecols:\n",
    "            tempcol=col+'xray'\n",
    "            if tempcol in allcols:\n",
    "                structured.loc[structured[tempcol]==1,col]=1\n",
    "                structured=structured.drop(tempcol,1)\n",
    "                \n",
    "        \n",
    "    #add ards_scale_1\n",
    "    structured=pd.merge(structured,ards[['EncounterID','ards_scale_1']],how='left',on='EncounterID')\n",
    "    \n",
    "\n",
    "    #add labels\n",
    "    #replace missing value with nan\n",
    "    ards.loc[ards['ards_time']=='.','ards_time']=np.nan\n",
    "    ards.ards_time=pd.to_datetime(ards.ards_time)\n",
    "\n",
    "    #merge with ards_time\n",
    "    structured=pd.merge(structured,ards[['EncounterID','ards_time']],how='left',on='EncounterID')\n",
    "\n",
    "    #if there is no ards_time, the patient had never had ards so the label will be 0\n",
    "    structured.loc[pd.isnull(structured['ards_time']),'ards']=0\n",
    "    #if the patient had ards,records after ards_time labeled as 1\n",
    "    structured.loc[(structured['ards_time']-timedelta(hours=0))<=structured['time'] ,'ards']=1\n",
    "    #if the patient had ards,records before ards_time labeled as 1\n",
    "    structured.loc[pd.isnull(structured['ards']),'ards']=0\n",
    "\n",
    "    structured=structured.drop(['ards_time'],1)\n",
    "\n",
    "\n",
    "    structured=sortdf(structured,structured[['EncounterID','time']],['EncounterID','time'])\n",
    "    \n",
    "    #deal with missing height and weight\n",
    "    #each encounter was assigned with 1 height and 1 weight\n",
    "    height=structured.groupby(['EncounterID','PatientID'])['height'].mean().reset_index(drop=False)\n",
    "\n",
    "    weight=structured.groupby(['EncounterID','PatientID'])['weight'].mean().reset_index(drop=False)\n",
    "    \n",
    "    age=structured.groupby(['EncounterID','PatientID'])['AgeInYears'].first().reset_index(drop=False)\n",
    "\n",
    "    gender=structured.groupby(['EncounterID','PatientID'])['GenderCode'].first().reset_index(drop=False)\n",
    "\n",
    "\n",
    "    structured=structured.drop(['height','weight','AgeInYears','GenderCode'],1)\n",
    "\n",
    "    structured=pd.merge(structured,height,how='left',on=['EncounterID','PatientID'])\n",
    "    structured=pd.merge(structured,weight,how='left',on=['EncounterID','PatientID'])\n",
    "    structured=pd.merge(structured,age,how='left',on=['EncounterID','PatientID'])\n",
    "    structured=pd.merge(structured,gender,how='left',on=['EncounterID','PatientID'])\n",
    "    \n",
    "    \n",
    "    ##calculate compliance and VR\n",
    "    usevtset=['VC+/AC'\n",
    "    ,'VC+/IMV'\n",
    "    ,'VC/AC'\n",
    "    ,'VC/IMV'\n",
    "    ,'VC/MMV']\n",
    "    usevtobs=['PC-IMV+/BiLevel'\n",
    "    ,'PC/PSV'\n",
    "    ,'PC/AC'\n",
    "    ,'PC/IMV'\n",
    "    ]\n",
    "\n",
    "    structured.loc[structured['vent_mode'].isin(usevtset),'Compliance']=structured['Vtset']/(structured['plat']-structured['peep'])\n",
    "    structured.loc[structured['vent_mode'].isin(usevtobs),'Compliance']=structured['Vte']/(structured['plat']-structured['peep'])\n",
    "    structured.loc[pd.isnull(structured['vent_mode']),'Compliance']=structured['Vtset']/(structured['plat']-structured['peep'])\n",
    "    structured.loc[(structured['Compliance']<0)|(structured['Compliance']>200),'Compliance']=np.nan\n",
    "\n",
    "    structured=structured.drop('vent_mode',1)\n",
    "    \n",
    "    #VR – Minute ventilation (VE) * PaCO2 * 1000 / (predicted body weight * 100 *37.5)\n",
    "    structured['VR']=structured['ve']*structured['paco2']*1000/(structured['weight']*100*37.5)\n",
    "    structured.loc[(structured['VR']<0.4)|(structured['VR']>10),'VR']=np.nan\n",
    "    \n",
    "    ####deal with missing data\n",
    "    structured.time=pd.to_datetime(structured.time)\n",
    "    #carry forward\n",
    "    carrydic={'temp':'8H','hr':'8H','rr':'8H','sbp':'8H','dbp':'8H','gcs':'24H','rass':'24H','shock_indx':'8H',\n",
    "\n",
    "             'spo2':'8H','fio2':'8H','pf':'48H','sf':'48H','support':'encounter',\n",
    "\n",
    "             'peep':'support','plat':'support','mairp':'support','ve':'support',\n",
    "\n",
    "            'o2flow_rate':'support', 'Vte':'support', 'Vtset':'support',\n",
    "\n",
    "            'Compliance':'support', 'VR':'support', 'oi':'support',\n",
    "\n",
    "            'lactate':'48H','ph':'48H','paco2':'48H','pao2': '48H',\n",
    "\n",
    "            'na': 'encounter','k': 'encounter','hco2': 'encounter','bun': 'encounter','cr':'encounter',\n",
    "\n",
    "             'alb':'encounter','tp':'encounter','tbili':'encounter','ast':'encounter','hgb': 'encounter','wbc':'encounter',\n",
    "\n",
    "             'plt': 'encounter','inr': 'encounter','ptt': 'encounter','bnp':'encounter','trop':'encounter','procalcitonin':'encounter',\n",
    "\n",
    "             'd-dimer':'encounter'}\n",
    "\n",
    "    for col in carrydic.keys():\n",
    "        #carry forward if the encounterid didn't change\n",
    "        if carrydic[col]=='encounter':\n",
    "            structured[col]=structured.groupby(['EncounterID'])[col].ffill()\n",
    "        #carry forward if the support type didn't change\n",
    "        elif carrydic[col]=='support':\n",
    "            structured[col]=structured.groupby(['EncounterID','support'])[col].ffill()\n",
    "        #carry forward N hours\n",
    "        else:\n",
    "            structured=carryhours(structured,col,int(carrydic[col][:-1]))\n",
    "            #structured[col]=structured.groupby(['EncounterID',pd.Grouper(key='time', freq=carrydic[col])])[col].ffill()\n",
    "\n",
    "\n",
    "    #set ards_scale_1\n",
    "    #if pt_ards==0\n",
    "    structured.loc[(structured['EncounterID'].isin(ards[ards['pt_ards']==0].EncounterID.unique()))&(~structured['support'].isin(['invasive','noninvasive','hfnc'])),'ards_scale_1']=1\n",
    "\n",
    "    #if pt_ards==1\n",
    "    structured.loc[(structured['EncounterID'].isin(ards[ards['pt_ards']==1].EncounterID.unique()))&(~structured['support'].isin(['invasive','noninvasive','hfnc']))&(structured['ards']==0),'ards_scale_1']=1\n",
    "    structured.loc[(structured['EncounterID'].isin(ards[ards['pt_ards']==1].EncounterID.unique()))&(structured['support'].isin(['invasive','noninvasive','hfnc']))&(structured['ards']==0),'ards_scale_1']=4\n",
    "    structured.loc[(structured['EncounterID'].isin(ards[ards['pt_ards']==1].EncounterID.unique()))&(~structured['support'].isin(['invasive','noninvasive','hfnc']))&(structured['ards']==1),'ards_scale_1']=4\n",
    "\n",
    "    #normalize the ards_scale_1\n",
    "    structured['ards_scale_1']=(structured['ards_scale_1']-min(structured['ards_scale_1']))/(max(structured['ards_scale_1'])-min(structured['ards_scale_1']))\n",
    "\n",
    "\n",
    "    ###bining the data \n",
    "\n",
    "    aggregation_functions = {}\n",
    "    floatcols=structured.loc[:, structured.dtypes == np.float64].columns.tolist()\n",
    "\n",
    "    \n",
    "    minmaxcols=['plat','mairp','pf','sf','oi','peep']\n",
    "    maxcols=['temp' ,'hr', 'rr', 'shock_indx' ,'fio2','o2flow_rate', 'bnp','procalcitonin','inr','fluid_bal','ve','paco2','VR','ptt','lactate','paco2','wbc', 'ddimer','trop']\n",
    "    mincols=['dbp','sbp','gcs','rass', 'spo2','alb','plt', 'tp','hgb','Vtset', 'pao2','Compliance','ph', 'na','k','hco2', 'ast']\n",
    "    meancols=['ards_scale_1','ra','null_percent','height','weight']\n",
    "    #set the aggregation function for binning the data\n",
    "    for col in structured.columns:\n",
    "        if col=='ards' :\n",
    "            aggregation_functions[col]='max'\n",
    "        elif include_orders and col in orderscols :\n",
    "            aggregation_functions[col]='max'\n",
    "        elif include_othernotes_merge and col in othercols:\n",
    "            aggregation_functions[col]='max'\n",
    "        elif (include_unstructured or include_radiology50) and col in unstructuredcols:\n",
    "            aggregation_functions[col]='max'\n",
    "        elif col in meancols:\n",
    "            aggregation_functions[col]='mean'\n",
    "        elif col in ['EncounterID','PatientID','time']:\n",
    "            continue\n",
    "        elif col in minmaxcols:\n",
    "            name=col+'_min'\n",
    "            aggregation_functions[name]='min'\n",
    "            name=col+'_max'\n",
    "            aggregation_functions[name]='max'\n",
    "        elif col in maxcols:\n",
    "            aggregation_functions[col]='max'\n",
    "        elif col in mincols:\n",
    "            aggregation_functions[col]='min'\n",
    "        elif col in floatcols:   \n",
    "            aggregation_functions[col]='min'\n",
    "        else:\n",
    "            aggregation_functions[col]='last'\n",
    "            \n",
    "    \n",
    "    for col in minmaxcols:\n",
    "        name=col+'_min'\n",
    "        structured[name]=structured[col].copy()\n",
    "        name=col+'_max'\n",
    "        structured[name]=structured[col].copy()\n",
    "        structured=structured.drop(col,1)\n",
    "    \n",
    "    print(aggregation_functions)\n",
    "\n",
    "    \n",
    "    structured_bin = structured.groupby(['EncounterID','PatientID',pd.Grouper(key='time', freq=(str(bintrain)+'H'))]).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "    structured_bin2 = structured.groupby(['EncounterID','PatientID',pd.Grouper(key='time', freq=(str(bintest)+'H'))]).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "\n",
    "    #fill in missing values with predefined values\n",
    "    fillin={'temp':98.3,'hr':80,'rr':20,'sbp':110,'dbp':60,'gcs':15,'rass':0,'shock_indx':0.7,\n",
    "            'spo2':98, 'fio2':21,'pf':400,'sf':400,'peep':0,'plat':5,'mairp':5,'ve':5,\n",
    "            'o2flow_rate':0,'Vte':400,'Vtset':400,'oi':1, 'Compliance':50,'VR':1,\n",
    "            'lactate':0,'ph':7.4,'paco2':40,'pao2':90,'na':140,\n",
    "            'k':4,'hco2':24,'bun':25,'hgb':12,'wbc':10,'plt':150,'inr':1,'ptt':1}\n",
    "    \n",
    "    for col in fillin.keys():\n",
    "        if col in minmaxcols:\n",
    "            name=col+'_min'\n",
    "            structured_bin[name]=structured_bin[name].fillna(fillin[col])\n",
    "            structured_bin2[name]=structured_bin2[name].fillna(fillin[col])\n",
    "            name=col+'_max'\n",
    "            structured_bin[name]=structured_bin[name].fillna(fillin[col])\n",
    "            structured_bin2[name]=structured_bin2[name].fillna(fillin[col])\n",
    "        else:\n",
    "            structured_bin[col]=structured_bin[col].fillna(fillin[col])\n",
    "            structured_bin2[col]=structured_bin2[col].fillna(fillin[col])\n",
    "\n",
    "    \n",
    "    \n",
    "    #train test split\n",
    "    trainset=ards[(ards['year']==2016)&(ards['not_reviewed']==0)&(ards['not_cohort']==0)&(pd.notnull(ards['pt_ards']))].EncounterID.unique().tolist()\n",
    "    testset=ards[(ards['year']==2017)&(ards['not_reviewed']==0)&(ards['not_cohort']==0)&(pd.notnull(ards['pt_ards']))].EncounterID.unique().tolist()\n",
    "\n",
    "\n",
    "    train_str,_=process(structured_bin,trainset,testset)\n",
    "    train_str2,test_str=process(structured_bin2,trainset,testset)\n",
    "\n",
    "    if len(train_str.columns)>=len(train_str2.columns):\n",
    "        train_str=train_str[train_str2.columns]\n",
    "    else:\n",
    "        train_str2=train_str2[train_str.columns]\n",
    "\n",
    "    test_str=test_str[train_str.columns] \n",
    "    \n",
    "    filename='structured'\n",
    "    if include_unstructured:\n",
    "        filename+='+unstructured'\n",
    "    if include_radiology50:\n",
    "        filename+='+radiology50'\n",
    "    if include_othernotes_merge:\n",
    "        filename+='+clinical_notes250'\n",
    "    if include_orders:\n",
    "        filename+='+medication'\n",
    "    \n",
    "   \n",
    "\n",
    "    train_str.to_csv(PATH5+filename+'_'+str(bintrain)+'Htrain.csv',index=False)\n",
    "    train_str2.to_csv(PATH5+filename+'_'+str(bintest)+'Htrain.csv',index=False)\n",
    "    test_str.to_csv(PATH5+filename+'_'+str(bintest)+'Htest.csv',index=False)\n",
    "    print(filename+'_'+str(bintrain)+'Htrain.csv')\n",
    "    print(filename+'_'+str(bintest)+'Htrain.csv')\n",
    "    print(filename+'_'+str(bintest)+'Htest.csv')\n",
    "    \n",
    "    return 'finished'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'include_unstructured' True if we want to add the unstructured data\n",
    "# 'include_othernotes_merge' True if we want to add the ctakes features from clinical notes\n",
    "# 'include_orders' True if we want to add the orders/medications\n",
    "# #Train the model on data binned every N hours\n",
    "# bintrain=6\n",
    "# #evaluate and test the model on data binned every N hours\n",
    "# bintest=2\n",
    "# 'clinicalnotes_only' True if we want to generate a dataset only have clinical notes ctakes features\n",
    "# 'ctakes_only' True if we want to generate a dataset only have chest x ray ctakes features\n",
    "# 'unigrambigram_only' True if we want to generate a dataset only have chest x ray unigram+bigram features\n",
    "# 'unstructured_only' True if we want to generate a dataset only have chest x ray keywords+sniffer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1='Z:\\patient-adjudication-results\\\\'\n",
    "PATH2='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\final_datasets\\\\'\n",
    "PATH3='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\'\n",
    "PATH4='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\model_outputs\\\\'\n",
    "PATH5='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\final_datasets\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters=[ #structured\n",
    "             {'include_unstructured':False,'include_othernotes_merge':False,'include_orders':False,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':False,'medication_only':False},\n",
    "            #structured+medication\n",
    "             {'include_unstructured':False,'include_othernotes_merge':False,'include_orders':True,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':False,'medication_only':False},\n",
    "            #structured+unstructured\n",
    "             {'include_unstructured':True,'include_othernotes_merge':False,'include_orders':False,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':False,'medication_only':False},\n",
    "            #Structured+radiology50\n",
    "            {'include_unstructured':False,'include_othernotes_merge':False,'include_orders':False,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':True,'medication_only':False},\n",
    "            #structured+radiology50+clinicalnotes250\n",
    "             {'include_unstructured':False,'include_othernotes_merge':True,'include_orders':False,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':True,'medication_only':False},\n",
    "            #structured+radiology50+clinicalnotes250+medication\n",
    "             {'include_unstructured':False,'include_othernotes_merge':True,'include_orders':True,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':True,'medication_only':False},\n",
    "            #Structured+unstructured+clinical notes c-takes\n",
    "            {'include_unstructured':True,'include_othernotes_merge':True,'include_orders':False,'bintrain':6,'bintest':2,'clinicalnotes_only':False, 'ctakes_only':False, 'unigrambigram_only':False, \n",
    "             'unstructured_only':False,'include_radiology50':False,'medication_only':False}\n",
    "           \n",
    "            \n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    print('***********',p)\n",
    "    preprocessing(**p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
