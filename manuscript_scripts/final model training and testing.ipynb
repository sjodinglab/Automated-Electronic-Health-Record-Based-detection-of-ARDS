{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree.export import export_graphviz\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pyodbc\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "%matplotlib inline\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interp\n",
    "import pickle\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model:fitted model\n",
    "#inputx:data to be predicted\n",
    "#inputdata: data to be predicted+labels\n",
    "#threshold: threshold used to get the predicted label. if predicted prob>threshold, predict 1\n",
    "#bootstrap: True if we are applying the function on bootstrapped dataset\n",
    "def getscores(model,inputx,inputdata,threshold,bootstrap=False):\n",
    "    \n",
    "    x=inputx.copy()\n",
    "    data=inputdata.copy()\n",
    "    data['predicted_prob']=model.predict_proba(x)[:,1]\n",
    "   \n",
    "    \n",
    "    if not bootstrap:\n",
    "        cali_y1, cali_x1 = calibration_curve(data['ards'],data['predicted_prob'],n_bins=10, normalize=True)\n",
    "        \n",
    "        aggregation_functions={'predicted_prob':'max','ards':'max'}\n",
    "        data2 = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "\n",
    "        #get calibration curve\n",
    "        cali_y2, cali_x2 = calibration_curve(data2['ards'],data2['predicted_prob'],n_bins=10, normalize=False)\n",
    "\n",
    "     \n",
    "    #get predicted label\n",
    "    data.loc[data['predicted_prob']>=threshold,'predicted']=1\n",
    "    data.loc[data['predicted_prob']<threshold,'predicted']=0\n",
    "\n",
    "    #ENCOUNTER level predition\n",
    "    \n",
    "    aggregation_functions={'predicted':'max','ards':'max'}\n",
    "    dataenc = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "    #get list of encounterid that the predicted label and true label are both 1\n",
    "    agree=dataenc[(dataenc['ards']==1 )&(dataenc['predicted']==1)].EncounterID.unique().tolist()\n",
    "    \n",
    "    data=data.sort_values(['EncounterID','time'], ascending=[True,True])\n",
    "    data_hours=data[data['EncounterID'].isin(agree)]\n",
    "    data_hours=data_hours[data_hours['predicted']==1]\n",
    "    \n",
    "\n",
    "    #get time difference\n",
    "    data_hours.time=pd.to_datetime(data_hours.time)\n",
    "    \n",
    "    data_hours.time=pd.to_datetime(data_hours.time)\n",
    "    data_hours=data_hours.groupby(['EncounterID'])['time'].first().reset_index(drop=False)\n",
    "    data_hours=pd.merge(data_hours,ards[['EncounterID','ards_time']],how='left',on='EncounterID')\n",
    "    data_hours.ards_time=pd.to_datetime(data_hours.ards_time)\n",
    "    data_hours['diff']=(((data_hours['time']+timedelta(hours=bintest)))-data_hours['ards_time'])/ np.timedelta64(1, 'h')\n",
    "    data_hours.loc[data_hours['diff']<=0,'earlydiff']=abs(data_hours['diff'])\n",
    "    data_hours.loc[data_hours['diff']>0,'latediff']=abs(data_hours['diff'])\n",
    "    #get median time difference when the prediction was made earlier than ards_time\n",
    "    early_avg_diff=np.nanmedian(data_hours['earlydiff'])\n",
    "    #get median time difference when the prediction was made later than ards_time\n",
    "    late_avg_diff=np.nanmedian(data_hours['latediff'])\n",
    "    #get percentage of early prediction\n",
    "    earlypct=len(data_hours[data_hours['diff']<=0])/len(data_hours)*100\n",
    "    #get percentage of late prediction\n",
    "    latepct=len(data_hours[data_hours['diff']>0])/len(data_hours)*100\n",
    "    \n",
    "    #avg_diff=(round(earlypct,2),round(early_avg_diff,2),round(latepct,2),round(late_avg_diff,2))\n",
    "    avg_diff=np.nanmedian(data_hours['diff'])\n",
    "    \n",
    "#     print(len(data_hours))\n",
    "#     print(len(dataenc[dataenc['ards']==1]))\n",
    "    if not bootstrap:\n",
    "        #save the data used to plot time curve\n",
    "        rows_list = []\n",
    "        for i in range(-48,49):\n",
    "            dic1 = {}\n",
    "            dic1['Time to ards_time(hours)']=i\n",
    "            dic1['Encounter(%)']=len(data_hours[data_hours['diff']<=i])/len(dataenc[dataenc['ards']==1])*100\n",
    "            rows_list.append(dic1)\n",
    "\n",
    "        timecurve = pd.DataFrame(rows_list) \n",
    "    \n",
    "    #get sensitivity, specificity and ppv\n",
    "    CM = confusion_matrix(dataenc['ards'],dataenc['predicted'])\n",
    "    tn, fp, fn, tp =CM.ravel()\n",
    "    \n",
    "    recall=tp/(tp+fn)\n",
    "    recall_ad=(tp+2)/(tp+fn+4)\n",
    "    sensitivity=recall\n",
    "    sen_ci=((sensitivity-1.96*sqrt(sensitivity*(1-sensitivity)/(tp+fn)))*100,(sensitivity+1.96*sqrt(sensitivity*(1-sensitivity)/(tp+fn)))*100)\n",
    "    \n",
    "    sp111=tn/(tn+fp)\n",
    "    sp111_a=(tn+2)/(tn+fp+4)\n",
    "    specificity=sp111\n",
    "    spe_ci=((specificity-1.96*sqrt(specificity*(1-specificity)/(tn+fp)))*100,(specificity+1.96*sqrt(specificity*(1-specificity)/(tn+fp)))*100)\n",
    "    \n",
    "    sp111=tp/(tp+fp)\n",
    "    sp111_a=(tp+2)/(tp+fp+4)\n",
    "    ppv=sp111\n",
    "    ppv_ci=((ppv-1.96*sqrt(ppv*(1-ppv)/(tp+fp)))*100,(ppv+1.96*sqrt(ppv*(1-ppv)/(tp+fp)))*100)\n",
    "    \n",
    "    \n",
    "    if not bootstrap:\n",
    "        return round(sensitivity*100,1), round(specificity*100,1),round(ppv*100,1),avg_diff,(cali_y1,cali_y2),(cali_x1,cali_x2),timecurve\n",
    "    else:\n",
    "        return round(sensitivity*100,1), round(specificity*100,1),round(ppv*100,1),avg_diff\n",
    "    \n",
    "#get bin rocauc and prc score\n",
    "def getroc(model,y,x):\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y, model.predict_proba(x)[:,1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    #prc\n",
    "    precision, recall, thresholds = precision_recall_curve(y, model.predict_proba(x)[:,1]) \n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    return round(roc_auc,3),round(pr_auc,3)\n",
    "\n",
    "#get encounter rocauc and prc score\n",
    "def getroc_encounter(model,inputx,inputdata,outputname=0,bootstrap=False):\n",
    "    x=inputx.copy()\n",
    "    data=inputdata.copy()\n",
    "    data['predicted']=model.predict_proba(x)[:,1]\n",
    "    if not bootstrap:\n",
    "        data[['EncounterID','time','ards','predicted']].to_csv(PATH4+outputname+'_predicted_proba.csv',index=False)\n",
    "    \n",
    "    \n",
    "    aggregation_functions={'predicted':'max','ards':'max'}\n",
    "    data = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "   \n",
    "    fpr, tpr, threshold = metrics.roc_curve(data['ards'],data['predicted'])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    #save the data to plot rocauc curve\n",
    "    if outputname!=0 and not bootstrap:\n",
    "        output=pd.DataFrame(columns=['fpr','tpr'])\n",
    "        output['fpr']=fpr\n",
    "        output['tpr']=tpr\n",
    "        output.to_csv(PATH4+outputname+'_rocauc.csv',index=False)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(data['ards'], data['predicted']) \n",
    "    #retrieve probability of being 1(in second column of probs_y)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    #save the data to plot precision recall curve\n",
    "    if outputname!=0 and not bootstrap:\n",
    "        output=pd.DataFrame(columns=['precision','recall'])\n",
    "        output['precision']=precision\n",
    "        output['recall']=recall\n",
    "        output.to_csv(PATH4+outputname+'_prc.csv',index=False)\n",
    "\n",
    "    if not bootstrap:\n",
    "        #plot Precision-Recall vs Threshold Chart\n",
    "        plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "        plt.plot(thresholds, precision[:-1] , \"b--\", label=\"Precision\")\n",
    "        plt.plot(thresholds, recall[:-1], \"r--\", label=\"Recall\")\n",
    "        plt.ylabel(\"Precision, Recall\")\n",
    "        plt.xlabel(\"Threshold\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.ylim([0,1])\n",
    "        plt.show()\n",
    "    \n",
    "  \n",
    "    return round(roc_auc, 3),round(pr_auc,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there is more than two columns to the data, you can just read in\n",
    "# the two columns of data, try to sort it, and then save the index of the\n",
    "# sorted dataframe. Then you can read in the original data and rearrange it\n",
    "# using the sorted index.\n",
    "def sortdf(data,temp,colstosort):\n",
    "\n",
    "    asc=[True]*len(colstosort)\n",
    "    temp=temp.sort_values(colstosort, ascending=asc)\n",
    "\n",
    "    index=temp.index\n",
    "\n",
    "    data=data.reindex(index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters:\n",
    "#model: sklearn model with defined hyperparameters\n",
    "#inputtraindata: the data used to train the model (binned every 6 hours)\n",
    "#inputtraindata2: the data used to test the model (binned every 2 hours)\n",
    "#lda: True if the model is lda\n",
    "#useweight: True if we want to use sampleweight\n",
    "def crossvalidate(model,inputtraindata,inputtraindata2,lda=False,useweight=True):\n",
    "    traindata=inputtraindata.copy()\n",
    "    traindata=traindata.drop('time',1)\n",
    "    \n",
    "    traindata2=inputtraindata2.copy()\n",
    "    traindata2=traindata2.drop('time',1)\n",
    "\n",
    "    #split the data by patientid\n",
    "    split=traindata[['PatientID','ards']].groupby(['PatientID']).sum().reset_index()\n",
    "    split.loc[split['ards']>0,'ards']=1\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "    skf.get_n_splits(split['PatientID'], split['ards'])\n",
    "\n",
    "    X,y=split['PatientID'], split['ards']\n",
    "    final=0\n",
    "    i=1\n",
    "    #inner cross validation\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print('inner',i)\n",
    "        i+=1\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        trainset, testset = X[train_index], X[test_index]\n",
    "        \n",
    "        X_train=traindata[traindata['PatientID'].isin(trainset)].drop(['EncounterID','PatientID','ards'],1)\n",
    "        sampleweight=X_train['sampleweight']\n",
    "        X_train=X_train.drop('sampleweight',1)\n",
    "        y_train=traindata[traindata['PatientID'].isin(trainset)]['ards']\n",
    "\n",
    "        X_val=traindata2[traindata2['PatientID'].isin(testset)].drop(['EncounterID','PatientID','ards','sampleweight'],1)\n",
    "        y_val=traindata2[traindata2['PatientID'].isin(testset)]['ards']\n",
    "\n",
    "        sc = StandardScaler()  \n",
    "\n",
    "        X_train_sc= sc.fit_transform(X_train)\n",
    "        X_val_sc= sc.transform (X_val)\n",
    "\n",
    "        if lda:\n",
    "            model.fit(X_train_sc, y_train)\n",
    "\n",
    "        else:\n",
    "            if useweight:\n",
    "                model.fit(X_train_sc, y_train,sample_weight=sampleweight)\n",
    "            else:\n",
    "                model.fit(X_train_sc, y_train)\n",
    "                \n",
    "        roc,prc=getroc(model,y_val,X_val_sc)\n",
    "        final=final+roc\n",
    "       \n",
    "        \n",
    "    final=final/5\n",
    "    print(final)\n",
    "     \n",
    "\n",
    "    return final\n",
    "\n",
    "#model: fitted model\n",
    "#inputx: data to be predicted\n",
    "#inputdata: data to be predicted+true labels\n",
    "def get_threshold(model,inputx,inputdata):\n",
    "    x=inputx.copy()\n",
    "    data=inputdata.copy()\n",
    "    #get predicted probability\n",
    "    data['predicted_prob']=model.predict_proba(x)[:,1]\n",
    "    \n",
    "    #get each encounter's max predicted_probability and labels\n",
    "    #ards==1 if the encounter had had ards, otherwise 0\n",
    "    aggregation_functions={'predicted_prob':'max','ards':'max'}\n",
    "    data = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "    \n",
    "    #get list of precision, recall, and corresponding thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(data['ards'], data['predicted_prob']) \n",
    "\n",
    "    \n",
    "    temp=pd.DataFrame(columns=['precision', 'recall', 'thresholds'])\n",
    "    temp['precision']=precision[: -1]\n",
    "    temp['recall']=recall[: -1]\n",
    "    temp['thresholds']=thresholds\n",
    "    temp=temp.sort_values(['recall'], ascending=[True])\n",
    "    temp=temp[temp['recall']>=0.85]\n",
    "    \n",
    "    if len(temp)>0:\n",
    "        final_threshold=temp['thresholds'].iloc[0]\n",
    "    else:\n",
    "        final_threshold=0\n",
    "     \n",
    "    return final_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(data):\n",
    "    dfs = [x for _, x in data.groupby('EncounterID')]\n",
    "    dfs2 = [random.choice(dfs) for _ in dfs]\n",
    "    data = pd.concat(dfs2)\n",
    "    #data=data.iloc[np.random.choice(len(data), size=len(data))]\n",
    "    return data\n",
    "\n",
    "def getci(data):\n",
    "    i=0\n",
    "    final={'rocauc':[],'prauc':[],'rocauc_enc':[],'prc_enc':[],'sen':[],'spe':[],'ppv':[],'avgdiff':[]}\n",
    "    while i<=1000:\n",
    "        print(i)\n",
    "        i+=1\n",
    "        temp=bootstrap(data)\n",
    "        X_sc= sc.transform(temp.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "        rocauc,prauc=getroc(clf,temp['ards'],X_sc)\n",
    "        rocauc_encounter,prauc_encounter=getroc_encounter(clf,X_sc,temp,outputname=0,bootstrap=True)\n",
    "        sensitivity,specificity, ppv,timediff=getscores(clf,X_sc,temp,final_threshold,bootstrap=True)\n",
    "\n",
    "        final['rocauc'].append(rocauc)\n",
    "        final['prauc'].append(prauc)\n",
    "        final['rocauc_enc'].append(rocauc_encounter)\n",
    "        final['prc_enc'].append(prauc_encounter)\n",
    "        final['sen'].append(sensitivity)\n",
    "        final['spe'].append(specificity)\n",
    "        final['ppv'].append(ppv)\n",
    "        final['avgdiff'].append(timediff)\n",
    "    \n",
    "    for k in final.keys():\n",
    "        final[k].sort()\n",
    "        final[k]=np.percentile(final[k], [2.5, 97.5])  \n",
    "        \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_intubated=True\n",
    "use_sample_weight=True\n",
    "choose_model='logistic regression'\n",
    "classweight=None #None or 'balanced'\n",
    "ifcalibrate=False\n",
    "#what data we want to use: structured, structured+unstructured, structured+unstructured+order, or structured+unstructured+clinicalnotes\n",
    "file='structured_allmedian'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1='Z:\\patient-adjudication-results\\\\'\n",
    "PATH2='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\final_datasets_testing\\\\'\n",
    "PATH3='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\'\n",
    "PATH4='Z:\\project-datasets\\ARDS\\ml_algorithms\\model_outputs_testing\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ards=pd.read_csv(PATH1+'current-ards-review-results_2_25_2020.csv',dtype={'mrn': str})\n",
    "ards.rename(columns={'encounterid':'EncounterID'},inplace=True)\n",
    "ards.loc[ards['ards_time']=='.','ards_time']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've settled on using data binned every 6h for training, and data binned every 2h for validation and testing\n",
    "#This script can run on any training data in the format of 'EncounterID','PatientID','ards','time','sampleweight',features\n",
    "if 'only' in file:\n",
    "    bintrain=0\n",
    "    bintest=0\n",
    "    filename1=PATH2+file+'_train.csv'\n",
    "    filename2=PATH2+file+'_train.csv'\n",
    "else:\n",
    "    filename1=PATH2+file+'_6Htrain.csv'\n",
    "    filename2=PATH2+file+'_2Htrain.csv'\n",
    "    bintrain=int(filename1.split('_')[-1][0])\n",
    "    bintest=int(filename2.split('_')[-1][0])\n",
    "    \n",
    "train_str=pd.read_csv(filename1)\n",
    "train_str2=pd.read_csv(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of encounters that have been intubated \n",
    "structured=pd.read_csv(PATH2+'structured_data.csv')\n",
    "intubated=structured[structured['support']=='invasive']\n",
    "intubated_encounters=intubated.EncounterID.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression\n",
    "parameters1 = {'penalty' : ['l1'] ,'C': [ 0.005,0.01,0.05,0.1],'class_weight':[classweight],'n_jobs':[-1],'random_state':[1234]}\n",
    "\n",
    "#random forest\n",
    "parameters2 = {'bootstrap': [True],\n",
    "'max_depth': [8,16,None],\n",
    "'max_features': ['auto'],\n",
    "'min_samples_leaf': [10],\n",
    "'min_samples_split': [50,100],\n",
    "'n_estimators': [400],\n",
    "'class_weight':[classweight],\n",
    "'random_state':[1234]}\n",
    "\n",
    "#lightgbm\n",
    "parameters3={'num_leaves':[8,16,32],\n",
    "             'max_depth':[4,8,-1],\n",
    "             'min_data_in_leaf':[50,100,200],\n",
    "             'n_estimators':[400],\n",
    "             'learning_rate':[0.01],\n",
    "             'objective':['binary'],\n",
    "             'subsample':[0.3,0.5],\n",
    "             'colsample_bytree':[0.3,0.5],\n",
    "             'random_state':[1234],\n",
    "             'n_jobs':[-1],\n",
    "             'reg_alpha':[0.3,0.5],\n",
    "             'silent':[False],\n",
    "             'class_weight':[classweight]\n",
    "            }\n",
    "\n",
    "#lda\n",
    "parameters4={'solver':['lsqr'],\n",
    "            'shrinkage':[None,'auto',0.2,0.4,0.6,0.8],\n",
    "            'n_components':[None]}\n",
    "\n",
    "paramdic={'logistic regression':parameters1,'random forest':parameters2,'lightgbm':parameters3,'lda':parameters4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#train model using 5 fold inner cross validation on the entire training set\n",
    "        \n",
    "#get list of hyperparameter combinations\n",
    "grid=list(ParameterGrid(paramdic[choose_model]))\n",
    "model=choose_model\n",
    "        \n",
    "#inner corss validation\n",
    "#crossvalidate() returns the averaged bin rocauc score after running 5 folds inner cross validation\n",
    "best_parameters={}\n",
    "for p in grid:\n",
    "    print(p)\n",
    "    if model=='logistic regression':\n",
    "        best_parameters[str(p)]=crossvalidate(LogisticRegression(**p),train_str,train_str2,useweight=use_sample_weight)\n",
    "    elif model=='lda':\n",
    "        best_parameters[str(p)]=crossvalidate(LinearDiscriminantAnalysis(**p),train_str,train_str2,lda=True,useweight=use_sample_weight)\n",
    "    elif model=='random forest':\n",
    "        best_parameters[str(p)]=crossvalidate(RandomForestClassifier(**p),train_str,train_str2,useweight=use_sample_weight)\n",
    "    elif model=='lightgbm':\n",
    "        best_parameters[str(p)]=crossvalidate(lgb.LGBMClassifier(**p),train_str,train_str2,useweight=use_sample_weight)\n",
    "            \n",
    "                \n",
    "\n",
    "#find the set of hyperparameters with the highest bin rocauc, retrain model on the full training set \n",
    "       \n",
    "best=max(best_parameters, key=best_parameters.get)\n",
    "\n",
    "sampleweight=train_str['sampleweight']\n",
    "\n",
    "#standardize\n",
    "sc = StandardScaler()  \n",
    "X_train_sc= sc.fit_transform(train_str.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "X_train_sc2= sc.transform(train_str2.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "\n",
    "\n",
    "#define models\n",
    "if model=='logistic regression':\n",
    "    clf = LogisticRegression(**ast.literal_eval(best))  \n",
    "           \n",
    "elif model=='lda':\n",
    "    clf = LinearDiscriminantAnalysis(**ast.literal_eval(best))\n",
    "\n",
    "elif model=='random forest':\n",
    "    clf = RandomForestClassifier(**ast.literal_eval(best))\n",
    "\n",
    "elif model=='lightgbm':\n",
    "    clf = lgb.LGBMClassifier(**ast.literal_eval(best))\n",
    "        \n",
    "#True if we want to calibrate the model\n",
    "if ifcalibrate:\n",
    "    clf=CalibratedClassifierCV(clf, method='sigmoid', cv=5)\n",
    "        \n",
    "#Fit the model to the entire training set\n",
    "#True if we want to use sampleweight\n",
    "if use_sample_weight and model!='lda':\n",
    "    clf.fit(X_train_sc, train_str['ards'],sampleweight)\n",
    "else:\n",
    "    clf.fit(X_train_sc, train_str['ards'])\n",
    "\n",
    "#output coefficients for logistic regression model\n",
    "if model=='logistic regression':\n",
    "    names=train_str.drop(['EncounterID','PatientID','ards','time','sampleweight'],1).columns\n",
    "    coefficients= pd.concat([pd.DataFrame(names),pd.DataFrame(np.transpose(clf.coef_))], axis = 1)\n",
    "    coefficients.to_csv(PATH4+'coefficients_'+choose_model+'_'+file+'.csv',index=False)\n",
    "    \n",
    "# Save to file in the current working directory\n",
    "pkl_filename = PATH4+choose_model+'_'+file+\"_model.pkl\"\n",
    "with open(pkl_filename, 'wb') as file1:\n",
    "    pickle.dump(clf, file1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('best hyperparameters:', best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_intubated:\n",
    "    sc = StandardScaler()  \n",
    "    train_str2=train_str2[train_str2['EncounterID'].isin(intubated_encounters)]\n",
    "    _= sc.fit_transform(train_str.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "    X_train_sc2= sc.transform(train_str2.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the threshold so the sensitivity==85%\n",
    "train_threshold=get_threshold(clf,X_train_sc2,train_str2)\n",
    "print('threshold:',train_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_str.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rocauc,prauc=getroc(clf,train_str2['ards'],X_train_sc2)\n",
    "rocauc_encounter,prauc_encounter=getroc_encounter(clf,X_train_sc2,train_str2,outputname=file+' '+choose_model+' train')\n",
    "sensitivity,specificity, ppv,timediff,caliy,calix,timecurve=getscores(clf,X_train_sc2,train_str2,train_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration plot encounter level\n",
    "for i in [0,1]:\n",
    "    fig = plt.figure(1, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "\n",
    "\n",
    "    ax1.plot(calix[i], caliy[i], \"s-\",\n",
    "                     label=\"%s\" % (choose_model))\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "    plt.show()\n",
    "    if i==0:\n",
    "        fig.savefig(PATH4+file+' '+choose_model+\"_train_calibration_bin.pdf\", bbox_inches='tight')\n",
    "    else:\n",
    "        fig.savefig(PATH4+file+' '+choose_model+\"_train_calibration_encounter.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'train score: ', rocauc,prauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'train score: ',rocauc_encounter,prauc_encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'train score: ',sensitivity, specificity, ppv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'train score: ', timediff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_intubated=True\n",
    "choose_model='logistic regression'\n",
    "file='structured'\n",
    "PATH1='Z:\\patient-adjudication-results\\\\'\n",
    "PATH2='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\final_datasets_alternative\\\\'\n",
    "PATH3='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\'\n",
    "PATH4='Z:\\project-datasets\\ARDS\\ml_algorithms\\model_outputs_alternative\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in test data\n",
    "\n",
    "if 'only' in file:\n",
    "    bintrain=0\n",
    "    bintest=0\n",
    "    filename1=PATH2+file+'_test.csv'\n",
    "    filename2=PATH2+file+'_train.csv'\n",
    "else:\n",
    "    \n",
    "    filename1=PATH2+file+'_2Htest.csv'\n",
    "    filename2=PATH2+file+'_6Htrain.csv'\n",
    "    bintest=int(filename1.split('_')[-1][0])\n",
    "    bintrain=int(filename2.split('_')[-1][0])\n",
    "\n",
    "testdata=pd.read_csv(filename1)\n",
    "traindata=pd.read_csv(filename2)\n",
    "ards=pd.read_csv(PATH1+'current-ards-review-results_2_25_2020.csv',dtype={'mrn': str})\n",
    "ards.rename(columns={'encounterid':'EncounterID'},inplace=True)\n",
    "ards.loc[ards['ards_time']=='.','ards_time']=np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of encounters that have been intubated \n",
    "structured=pd.read_csv(PATH2+'structured_data.csv')\n",
    "intubated=structured[structured['support']=='invasive']\n",
    "intubated_encounters=intubated.EncounterID.unique().tolist()\n",
    "if test_on_intubated:\n",
    "    testdata=testdata[testdata['EncounterID'].isin(intubated_encounters)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize\n",
    "sc = StandardScaler()  \n",
    "_=sc.fit_transform(traindata.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "X_test_sc= sc.transform(testdata.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model from disk\n",
    "pkl_filename = PATH4+choose_model+'_'+file+\"_model.pkl\"\n",
    "clf= pickle.load(open(pkl_filename, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get threshold\n",
    "final_threshold=get_threshold(clf,X_test_sc,testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the test scores\n",
    "rocauc,prauc=getroc(clf,testdata['ards'],X_test_sc)\n",
    "rocauc_encounter,prauc_encounter=getroc_encounter(clf,X_test_sc,testdata,outputname=file+' '+choose_model)\n",
    "sensitivity,specificity,ppv,timediff,caliy,calix,timecurve=getscores(clf,X_test_sc,testdata,final_threshold)\n",
    "timecurve.to_csv(PATH4+file+' '+choose_model+'_timecurve.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'test score: ',rocauc,prauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'test score: ',rocauc_encounter,prauc_encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'test score: ',sensitivity, specificity,ppv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'test score: ',timediff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calibration plot at encounter level\n",
    "for i in [0,1]:\n",
    "    fig = plt.figure(1, figsize=(10, 10))\n",
    "    ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "\n",
    "    ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "\n",
    "\n",
    "    ax1.plot(calix[i], caliy[i], \"s-\",\n",
    "                     label=\"%s\" % (choose_model))\n",
    "\n",
    "    ax1.set_ylabel(\"Fraction of positives\")\n",
    "    ax1.set_ylim([-0.05, 1.05])\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.set_title('Calibration plots  (reliability curve)')\n",
    "    plt.show()\n",
    "    if i==0:\n",
    "        fig.savefig(PATH4+file+' '+choose_model+\"_calibration_bin.pdf\", bbox_inches='tight')\n",
    "    else:\n",
    "        fig.savefig(PATH4+file+' '+choose_model+\"_calibration_encounter.pdf\", bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###time curve\n",
    "filenames=[PATH4+file+' '+choose_model+'_timecurve.csv']\n",
    "n_classes=len(filenames)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(np.arange(-48, 48, 12))\n",
    "ax.set_yticks(np.arange(0, 100, 10))\n",
    "plt.xlabel('Time to ards_time(hours)')\n",
    "plt.ylabel('Encounter(%)')\n",
    "colors=['red','green','blue','orange','purple','gray']\n",
    "for i in range(n_classes):\n",
    "    temp=pd.read_csv(filenames[i])\n",
    "    plt.plot(temp['Time to ards_time(hours)'], temp['Encounter(%)'],\n",
    "             label=filenames[i].split('\\\\')[-1].split('_')[0],color=colors[i],linewidth=0.8,linestyle='-')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig.savefig(PATH4+file+' '+choose_model+\"_timecurve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc auc\n",
    "filenames=[PATH4+file+' '+choose_model+'_rocauc.csv']\n",
    "n_classes=len(filenames)\n",
    "fpr={}\n",
    "tpr={}\n",
    "roc_auc ={}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    temp=pd.read_csv(filenames[i])\n",
    "    fpr[i], tpr[i]=temp['fpr'],temp['tpr']\n",
    "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "    \n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure()\n",
    "\n",
    "colors=['red','green','blue','orange','purple','gray']\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i],\n",
    "             label=filenames[i].split('\\\\')[-1].split('_')[0]+'(area = {0:0.4f})'\n",
    "                   ''.format(roc_auc[i]),\n",
    "             color=colors[i],  linewidth=0.8,linestyle='-')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC AUC Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "fig.savefig(PATH4+file+' '+choose_model+\"_rocauc.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prc\n",
    "filenames=[PATH4+file+' '+choose_model+'_prc.csv']\n",
    "n_classes=len(filenames)\n",
    "precision={}\n",
    "recall={}\n",
    "prc ={}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    temp=pd.read_csv(filenames[i])\n",
    "    precision[i], recall[i]=temp['precision'],temp['recall']\n",
    "    prc[i] = metrics.auc( recall[i],precision[i])\n",
    "    \n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure()\n",
    "\n",
    "colors=['red','green','blue','orange','purple','gray']\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i],precision[i], \n",
    "             label=filenames[i].split('\\\\')[-1].split('_')[0]+'(area = {0:0.4f})'\n",
    "                   ''.format(prc[i]),\n",
    "             color=colors[i],  linewidth=0.8,linestyle='-')\n",
    "\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "fig.savefig(PATH4+file+' '+choose_model+\"_prc.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 95% confidence interval\n",
    "CI=getci(testdata)\n",
    "\n",
    "#print out 95% confidence interval\n",
    "print('95% confidence interval for test scores')\n",
    "for k in CI.keys():\n",
    "    print(k,' ',CI[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
