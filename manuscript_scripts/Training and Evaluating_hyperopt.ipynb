{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import timeit\n",
    "import csv\n",
    "from timeit import default_timer as timer\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree.export import export_graphviz\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pyodbc\n",
    "from datetime import timedelta\n",
    "import ast\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.calibration import calibration_curve\n",
    "%matplotlib inline\n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from scipy import stats\n",
    "import lightgbm as lgb\n",
    "from matplotlib import pyplot\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from scipy import interp\n",
    "import pickle\n",
    "from math import sqrt\n",
    "from hyperopt import fmin\n",
    "from hyperopt import Trials\n",
    "from hyperopt import tpe\n",
    "from hyperopt import STATUS_OK\n",
    "from hyperopt import hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model:fitted model\n",
    "#inputx:data to be predicted\n",
    "#inputdata: data to be predicted+labels\n",
    "#threshold: threshold used to get the predicted label. if predicted prob>threshold, predict 1\n",
    "#same: same equal to the same parameter in function nested()\n",
    "def getscores(model,inputx,inputdata,threshold,same):\n",
    "    \n",
    "    x=inputx.copy()\n",
    "    data=inputdata.copy()\n",
    "    data['predicted_prob']=model.predict_proba(x)[:,1]\n",
    "   \n",
    "    #get calibration curve\n",
    "    cali_y, cali_x = calibration_curve(data['ards'],data['predicted_prob'],n_bins=10, normalize=True)\n",
    "     \n",
    "    #get predicted label\n",
    "    data.loc[data['predicted_prob']>=threshold,'predicted']=1\n",
    "    data.loc[data['predicted_prob']<threshold,'predicted']=0\n",
    "\n",
    "    #ENCOUNTER level predition\n",
    "    \n",
    "    aggregation_functions={'predicted':'max','ards':'max'}\n",
    "    dataenc = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "    #get list of encounterid that the predicted label and true label are both 1\n",
    "    agree=dataenc[(dataenc['ards']==1 )&(dataenc['predicted']==1)].EncounterID.unique().tolist()\n",
    "    \n",
    "    data=data.sort_values(['EncounterID','time'], ascending=[True,True])\n",
    "    data_hours=data[data['EncounterID'].isin(agree)]\n",
    "    data_hours=data_hours[data_hours['predicted']==1]\n",
    "    \n",
    "\n",
    "    #get time difference\n",
    "    data_hours.time=pd.to_datetime(data_hours.time)\n",
    "    \n",
    "    data_hours.time=pd.to_datetime(data_hours.time)\n",
    "    data_hours=data_hours.groupby(['EncounterID'])['time'].first().reset_index(drop=False)\n",
    "    data_hours=pd.merge(data_hours,ards[['EncounterID','ards_time']],how='left',on='EncounterID')\n",
    "    data_hours.ards_time=pd.to_datetime(data_hours.ards_time)\n",
    "    if same:\n",
    "        data_hours['diff']=(((data_hours['time']+timedelta(hours=bintrain)))-data_hours['ards_time'])/ np.timedelta64(1, 'h')\n",
    "    else:\n",
    "        data_hours['diff']=(((data_hours['time']+timedelta(hours=bintest)))-data_hours['ards_time'])/ np.timedelta64(1, 'h')\n",
    "    data_hours.loc[data_hours['diff']<=0,'earlydiff']=abs(data_hours['diff'])\n",
    "    data_hours.loc[data_hours['diff']>0,'latediff']=abs(data_hours['diff'])\n",
    "    #get median time difference when the prediction was made earlier than ards_time\n",
    "    early_avg_diff=np.nanmedian(data_hours['earlydiff'])\n",
    "    #get median time difference when the prediction was made later than ards_time\n",
    "    late_avg_diff=np.nanmedian(data_hours['latediff'])\n",
    "    #get percentage of early prediction\n",
    "    earlypct=len(data_hours[data_hours['diff']<=0])/len(data_hours)*100\n",
    "    #get percentage of late prediction\n",
    "    latepct=len(data_hours[data_hours['diff']>0])/len(data_hours)*100\n",
    "    \n",
    "    avg_diff=(round(earlypct,2),round(early_avg_diff,2),round(latepct,2),round(late_avg_diff,2))\n",
    "    \n",
    "    \n",
    "    #save the data used to plot time curve\n",
    "    rows_list = []\n",
    "    for i in range(-48,49):\n",
    "        dic1 = {}\n",
    "        dic1['Time to ards_time(hours)']=i\n",
    "        dic1['Encounter(%)']=len(data_hours[data_hours['diff']<=i])/len(dataenc[dataenc['ards']==1])*100\n",
    "        rows_list.append(dic1)\n",
    "\n",
    "    timecurve = pd.DataFrame(rows_list) \n",
    "    \n",
    "    #get sensitivity, specificity and ppv\n",
    "    CM = confusion_matrix(dataenc['ards'],dataenc['predicted'])\n",
    "    tn, fp, fn, tp =CM.ravel()\n",
    "    \n",
    "    recall=tp/(tp+fn)\n",
    "    recall_ad=(tp+2)/(tp+fn+4)\n",
    "    sensitivity=recall\n",
    "    \n",
    "    sp111=tn/(tn+fp)\n",
    "    sp111_a=(tn+2)/(tn+fp+4)\n",
    "    specificity=sp111\n",
    "    \n",
    "    sp111=tp/(tp+fp)\n",
    "    sp111_a=(tp+2)/(tp+fp+4)\n",
    "    ppv=sp111\n",
    "    \n",
    "    \n",
    "    return round(sensitivity*100,1), round(specificity*100,1),round(ppv*100,1),avg_diff,cali_y,cali_x,timecurve\n",
    "\n",
    "#get bin rocauc and prc score\n",
    "def getroc(model,y,x):\n",
    "    \n",
    "    fpr, tpr, threshold = metrics.roc_curve(y, model.predict_proba(x)[:,1])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    \n",
    "    #prc\n",
    "    precision, recall, thresholds = precision_recall_curve(y, model.predict_proba(x)[:,1]) \n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    return round(roc_auc,3),round(pr_auc,3)\n",
    "\n",
    "#get encounter rocauc and prc score\n",
    "def getroc_encounter(model,inputx,inputdata,outputname=0):\n",
    "    x=inputx.copy()\n",
    "    data=inputdata.copy()\n",
    "    data['predicted']=model.predict_proba(x)[:,1]\n",
    "    \n",
    "    \n",
    "    aggregation_functions={'predicted':'max','ards':'max'}\n",
    "    data = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "   \n",
    "    fpr, tpr, threshold = metrics.roc_curve(data['ards'],data['predicted'])\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    #save the data to plot rocauc curve\n",
    "    if outputname!=0:\n",
    "        output=pd.DataFrame(columns=['fpr','tpr'])\n",
    "        output['fpr']=fpr\n",
    "        output['tpr']=tpr\n",
    "        output.to_csv(PATH4+outputname+'_rocauc.csv',index=False)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(data['ards'], data['predicted']) \n",
    "    #retrieve probability of being 1(in second column of probs_y)\n",
    "    pr_auc = metrics.auc(recall, precision)\n",
    "    \n",
    "    #save the data to plot precision recall curve\n",
    "    if outputname!=0:\n",
    "        output=pd.DataFrame(columns=['precision','recall'])\n",
    "        output['precision']=precision\n",
    "        output['recall']=recall\n",
    "        output.to_csv(PATH4+outputname+'_prc.csv',index=False)\n",
    "\n",
    "    #plot Precision-Recall vs Threshold Chart\n",
    "    plt.title(\"Precision-Recall vs Threshold Chart\")\n",
    "    plt.plot(thresholds, precision[: -1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recall[: -1], \"r--\", label=\"Recall\")\n",
    "    plt.ylabel(\"Precision, Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"lower left\")\n",
    "    plt.ylim([0,1])\n",
    "    plt.show()\n",
    "    \n",
    "  \n",
    "    return round(roc_auc, 3),round(pr_auc,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming there is more than two columns to the data, you can just read in\n",
    "# the two columns of data, try to sort it, and then save the index of the\n",
    "# sorted dataframe. Then you can read in the original data and rearrange it\n",
    "# using the sorted index.\n",
    "def sortdf(data,temp,colstosort):\n",
    "\n",
    "    asc=[True]*len(colstosort)\n",
    "    temp=temp.sort_values(colstosort, ascending=asc)\n",
    "\n",
    "    index=temp.index\n",
    "\n",
    "    data=data.reindex(index)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters:\n",
    "#model: sklearn model with defined hyperparameters\n",
    "#inputtraindata: the data used to train the model (binned every 6 hours)\n",
    "#inputtraindata2: the data used to test the model (binned every 2 hours)\n",
    "#lda: True if the model is lda\n",
    "#useweight: True if we want to use sampleweight\n",
    "def crossvalidate(model,inputtraindata,inputtraindata2,lda=False,useweight=True):\n",
    "    traindata=inputtraindata.copy()\n",
    "    traindata=traindata.drop('time',1)\n",
    "    \n",
    "    traindata2=inputtraindata2.copy()\n",
    "    traindata2=traindata2.drop('time',1)\n",
    "\n",
    "    #split the data by patientid\n",
    "    split=traindata[['PatientID','ards']].groupby(['PatientID']).sum().reset_index()\n",
    "    split.loc[split['ards']>0,'ards']=1\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "    skf.get_n_splits(split['PatientID'], split['ards'])\n",
    "\n",
    "    X,y=split['PatientID'], split['ards']\n",
    "    final=0\n",
    "    i=1\n",
    "    #inner cross validation\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print('inner',i)\n",
    "        i+=1\n",
    "        #print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        trainset, testset = X[train_index], X[test_index]\n",
    "        \n",
    "        X_train=traindata[traindata['PatientID'].isin(trainset)].drop(['EncounterID','PatientID','ards'],1)\n",
    "        sampleweight=X_train['sampleweight']\n",
    "        X_train=X_train.drop('sampleweight',1)\n",
    "        y_train=traindata[traindata['PatientID'].isin(trainset)]['ards']\n",
    "\n",
    "        X_val=traindata2[traindata2['PatientID'].isin(testset)].drop(['EncounterID','PatientID','ards','sampleweight'],1)\n",
    "        y_val=traindata2[traindata2['PatientID'].isin(testset)]['ards']\n",
    "\n",
    "        sc = StandardScaler()  \n",
    "\n",
    "        X_train_sc= sc.fit_transform(X_train)\n",
    "        X_val_sc= sc.transform (X_val)\n",
    "\n",
    "        if lda:\n",
    "            model.fit(X_train_sc, y_train)\n",
    "\n",
    "        else:\n",
    "            if useweight:\n",
    "                model.fit(X_train_sc, y_train,sample_weight=sampleweight)\n",
    "            else:\n",
    "                model.fit(X_train_sc, y_train)\n",
    "                \n",
    "        roc,prc=getroc(model,y_val,X_val_sc)\n",
    "        final=final+roc\n",
    "       \n",
    "        \n",
    "    final=final/5\n",
    "    print(final)\n",
    "     \n",
    "\n",
    "    return final\n",
    "\n",
    "#model: fitted model\n",
    "#inputx: data to be predicted\n",
    "#inputdata: data to be predicted+true labels\n",
    "def get_threshold(model,inputx,inputdata):\n",
    "    x=inputx.copy()\n",
    "    data=inputdata.copy()\n",
    "    #get predicted probability\n",
    "    data['predicted_prob']=model.predict_proba(x)[:,1]\n",
    "    \n",
    "    #get each encounter's max predicted_probability and labels\n",
    "    #ards==1 if the encounter had had ards, otherwise 0\n",
    "    aggregation_functions={'predicted_prob':'max','ards':'max'}\n",
    "    data = data.groupby(['EncounterID']).aggregate(aggregation_functions).reset_index(drop=False)\n",
    "    \n",
    "    #get list of precision, recall, and corresponding thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(data['ards'], data['predicted_prob']) \n",
    "\n",
    "    \n",
    "    temp=pd.DataFrame(columns=['precision', 'recall', 'thresholds'])\n",
    "    temp['precision']=precision[: -1]\n",
    "    temp['recall']=recall[: -1]\n",
    "    temp['thresholds']=thresholds\n",
    "    temp=temp.sort_values(['recall'], ascending=[True])\n",
    "    temp=temp[temp['recall']>=0.85]\n",
    "    \n",
    "    if len(temp)>0:\n",
    "        final_threshold=temp['thresholds'].iloc[0]\n",
    "    else:\n",
    "        final_threshold=0\n",
    "     \n",
    "    return final_threshold\n",
    "\n",
    "#parameters:\n",
    "#model: what model we want to use? e.g. \"logistic regression\",'random forest','lightgbm','lda'\n",
    "#inputdata: the data used to train the model (binned every 6 hours)\n",
    "#inputdata2: the data used to test the model (binned every 2 hours)\n",
    "#same =True if inputdata==inputdata2\n",
    "#parameters: set of hyperparameters\n",
    "#outputname: the name of output files. need to read back in later for plotting all the models together\n",
    "#isintubated: True if we want to test on intubated patients only\n",
    "#useweight: True if we want to use sample weight\n",
    "#calibrate: True if we want to calibrate the output probability\n",
    "#inputintubated: subset of input data that only contains intubated patients; 0 if isintubated==False\n",
    "def nested(model,inputdata,inputdata2,same, outputname,isintubated=False,useweight=True,calibrate=False,inputintubated=0):\n",
    "    \n",
    "    data=inputdata.copy()\n",
    "    data2=inputdata2.copy()\n",
    "    intubated=inputintubated.copy()\n",
    "    \n",
    "    #if we want to test on intubated patients only, the intubated patientsID will be split into 5 folds\n",
    "    #patients in each fold will be used as test set so the test set only includes intubated patients\n",
    "    #patients not in that fold will be used as train set no matter they have been intubated or not\n",
    "    if isintubated:\n",
    "        split=intubated[['PatientID','ards']].groupby(['PatientID']).sum().reset_index()\n",
    "        split.loc[split['ards']>0,'ards']=1\n",
    "        \n",
    "    else:\n",
    "        # test on all the patients no matter they are intubated or not\n",
    "        split=data[['PatientID','ards']].groupby(['PatientID']).sum().reset_index()\n",
    "        split.loc[split['ards']>0,'ards']=1\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=1)\n",
    "    skf.get_n_splits(split['PatientID'], split['ards'])\n",
    "    X,y=split['PatientID'], split['ards']\n",
    "    \n",
    "    \n",
    "    \n",
    "    #lists to store the outputs when looping through each fold\n",
    "    #The outer cross validation has 5 folds so the length of list is 5\n",
    "    final1=[] #bin rocauc\n",
    "    final2=[] #enc rocauc\n",
    "    final11=[] #bin prauc\n",
    "    final22=[] #enc prauc\n",
    "    final3=[] #sensitivity\n",
    "    final4=[] #specificity\n",
    "    final5=[] #ppv\n",
    "    final6=[] #time diff (perc of early prediction, median timediff in hours,perc of late prediction, median timediff in hours)\n",
    "    final7=[] #calibration plot  y axix\n",
    "    final8=[] #calibration plot x axix\n",
    "    final9=[] # dataframe used to plot the timecurve\n",
    "    bestparam=[] #best set of hyperparameter for current training set\n",
    "    trainauc1=[] # train set's bin rocauc \n",
    "    trainauc2=[] # train set's enc rocauc\n",
    "  \n",
    "   \n",
    "  \n",
    "    ###variables needed for plotting rocauc curve\n",
    "    n_classes=5\n",
    "    fpr={}\n",
    "    tpr={}\n",
    "    precision={}\n",
    "    recall={}\n",
    "    \n",
    "    global trainset\n",
    "    global trainset2\n",
    "    i=0\n",
    "    #outer cross validation\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        print('************outer',i)\n",
    "        \n",
    "        #get patientid for trainset, patientID for testset\n",
    "        trainindex, testindex = X[train_index], X[test_index]\n",
    "\n",
    "        #trainset is used for training in inner cross validation\n",
    "        trainset=data[~(data['PatientID'].isin(testindex))]\n",
    "        #trainset2 is used for testing in inner cross validation\n",
    "        trainset2=data2[~(data2['PatientID'].isin(testindex))]\n",
    "        #testset is used for testing in outer cross validation\n",
    "        testset=data2[data2['PatientID'].isin(testindex)]\n",
    "        #We always train on every 6 hours and test on every 2 hours no matter it's inner or outer crossvalidation \n",
    "        \n",
    "        #inner corss validation\n",
    "        #find the set of hyperparameters with the highest bin rocauc, retrain model on the full training set \n",
    "       \n",
    "        #run Bayesian optimization\n",
    "        # Optimize\n",
    "        if model=='logistic regression':\n",
    "            tempspace=space1\n",
    "        elif model=='random forest':\n",
    "            tempspace=space2\n",
    "        elif model=='lightgbm':\n",
    "            tempspace=space3\n",
    "        elif model=='lda':\n",
    "            tempspace=space4\n",
    "            \n",
    "        #configure hyperopt\n",
    "        global  ITERATION\n",
    "        #Optimization Algorithm\n",
    "        global tpe_algorithm\n",
    "        # Trials object to track progress\n",
    "        global bayes_trials\n",
    "        \n",
    "        best = fmin(fn = objective, space = tempspace, algo = tpe_algorithm , \n",
    "                    max_evals = MAX_EVALS, trials = bayes_trials)\n",
    "        \n",
    "        \n",
    "        historydata=pd.read_csv( PATH4+'trials_'+file+'_'+choose_model+'_nested.csv')\n",
    "        historydata=historydata.sort_values(by='loss',ascending=True).reset_index(drop=True)\n",
    "\n",
    "        best=historydata['params'].iloc[0]\n",
    "\n",
    "        sampleweight=trainset['sampleweight']\n",
    "\n",
    "        #standardize\n",
    "        sc = StandardScaler()  \n",
    "        X_train_sc= sc.fit_transform(trainset.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "        X_test_sc= sc.transform (testset.drop(['EncounterID','PatientID','ards','time','sampleweight'],1))\n",
    "\n",
    "        #define models\n",
    "        if model=='logistic regression':\n",
    "            clf = LogisticRegression(**ast.literal_eval(best))  \n",
    "           \n",
    "        elif model=='lda':\n",
    "            clf = LinearDiscriminantAnalysis(**ast.literal_eval(best))\n",
    "\n",
    "        elif model=='random forest':\n",
    "            clf = RandomForestClassifier(**ast.literal_eval(best))\n",
    "\n",
    "        elif model=='lightgbm':\n",
    "            clf = lgb.LGBMClassifier(**ast.literal_eval(best))\n",
    "        \n",
    "        #True if we want to calibrate the model\n",
    "        if calibrate:\n",
    "            clf=CalibratedClassifierCV(clf, method='sigmoid', cv=5)\n",
    "        \n",
    "        #Fit the model to training set\n",
    "        #True if we want to use sampleweight\n",
    "        if useweight and model!='lda':\n",
    "            clf.fit(X_train_sc, trainset['ards'],sampleweight)\n",
    "        else:\n",
    "            clf.fit(X_train_sc, trainset['ards'])\n",
    "                \n",
    "        \n",
    "        #get the test scores\n",
    "        rocauc,prauc=getroc(clf,testset['ards'],X_test_sc)\n",
    "        rocauc_encounter,prauc_encounter=getroc_encounter(clf,X_test_sc,testset,outputname=outputname)\n",
    "        #get the train scores\n",
    "        rocauc_train,prauc_train=getroc(clf,trainset['ards'],X_train_sc)\n",
    "        rocauc_encounter_train,prauc_encounter_train=getroc_encounter(clf,X_train_sc,trainset)\n",
    "        \n",
    "        #save each fold's fpr,tpr precision,recall to dictionaries\n",
    "        #will be used later to get avaeraged rocauc curve and precison recall curve\n",
    "        temp=pd.read_csv(PATH4+outputname+'_rocauc.csv')\n",
    "        fpr[i], tpr[i]=temp['fpr'],temp['tpr']\n",
    "        \n",
    "        temp=pd.read_csv(PATH4+outputname+'_prc.csv')\n",
    "        precision[i], recall[i]=temp['precision'],temp['recall']\n",
    "  \n",
    "        i+=1\n",
    "    \n",
    "        #get the threshold so the test set sensitivity==85%\n",
    "        final_threshold_test=get_threshold(clf,X_test_sc,testset)\n",
    "        \n",
    "        final_threshold_train=get_threshold(clf,X_train_sc,trainset)\n",
    "    \n",
    "               \n",
    "        print('final_threshold',final_threshold_test)\n",
    "        #get the test scores\n",
    "        sensitivity, specificity, ppv,timediff,caliy,calix,timecurve=getscores(clf,X_test_sc,testset,final_threshold_test,same)\n",
    "        #get the train scores\n",
    "        sensitivity_train, specificity_train, ppv_train,timediff_train,caliy_train,calix_train,timecurve_train=getscores(clf,X_train_sc,trainset,final_threshold_train,same)\n",
    "     \n",
    "        bestparam.append(best)\n",
    "        final1.append(rocauc)\n",
    "        final2.append(rocauc_encounter)\n",
    "        final11.append(prauc)\n",
    "        final22.append(prauc_encounter)\n",
    "        final3.append(sensitivity)\n",
    "        final4.append(specificity)\n",
    "        final5.append(ppv)\n",
    "        final6.append(timediff)\n",
    "        final7.append(caliy)\n",
    "        final8.append(calix)\n",
    "        final9.append(timecurve)\n",
    "        trainauc1.append(rocauc_train)\n",
    "        trainauc2.append(rocauc_encounter_train)\n",
    "        \n",
    "        \n",
    "        print('*************',best,rocauc,rocauc_encounter,prauc,prauc_encounter,sensitivity, specificity, ppv,timediff)\n",
    "        print('*************train',rocauc_train,rocauc_encounter_train,prauc_train,prauc_encounter_train,sensitivity_train, specificity_train, ppv_train,timediff_train)\n",
    "\n",
    "        \n",
    "        #configure hyperopt\n",
    "        ITERATION = 0\n",
    "        #Optimization Algorithm\n",
    "        tpe_algorithm = tpe.suggest\n",
    "        # Trials object to track progress\n",
    "        bayes_trials = Trials()\n",
    "\n",
    "        #save result history\n",
    "        # File to save first results\n",
    "        out_file = PATH4+'trials_'+file+'_'+choose_model+'_nested.csv'\n",
    "        of_connection = open(out_file, 'w')\n",
    "        writer = csv.writer(of_connection)\n",
    "\n",
    "        # Write the headers to the file\n",
    "        writer.writerow(['loss', 'params', 'iteration','train_time'])\n",
    "        of_connection.close()\n",
    "\n",
    "    #plot averaged calibration curve            \n",
    "    try:\n",
    "        print(\"################averaged calibration plot\")\n",
    "        #calibration plot\n",
    "        y=np.zeros(shape=(10))\n",
    "        x=np.zeros(shape=(10))\n",
    "        for i in range(5):\n",
    "            y=y+np.array(final7[i])/5\n",
    "            x=x+np.array(final8[i])/5\n",
    "            \n",
    "        fig = plt.figure(1, figsize=(10, 10))\n",
    "        ax1 = plt.subplot2grid((3, 1), (0, 0), rowspan=2)\n",
    "        #ax2 = plt.subplot2grid((3, 1), (2, 0))\n",
    "\n",
    "        ax1.plot([0, 1], [0, 1], \"k:\", label=\"Perfectly calibrated\")\n",
    "        \n",
    "        fraction_of_positives, mean_predicted_value = caliy,calix\n",
    "\n",
    "        ax1.plot(x, y, \"s-\",\n",
    "                 label=\"%s\" % (model))\n",
    "\n",
    "        ax1.set_ylabel(\"Fraction of positives\")\n",
    "        ax1.set_ylim([-0.05, 1.05])\n",
    "        ax1.legend(loc=\"lower right\")\n",
    "        ax1.set_title('Calibration plots  (reliability curve)')\n",
    "        plt.show()\n",
    "        fig.savefig(PATH4+outputname+\"_calibration_bin.pdf\", bbox_inches='tight')\n",
    "\n",
    "    except:\n",
    "        print('calibration plot error')\n",
    "        \n",
    "    #plot averaged time curve\n",
    "    print(\"################averaged time curve\")\n",
    "    #time curve\n",
    "    timex=np.array(final9[0]['Time to ards_time(hours)'])\n",
    "    timey=np.array(final9[0]['Encounter(%)'])\n",
    "    for i in range(1,len(final9)):\n",
    "        timex=np.array(final9[i]['Time to ards_time(hours)'])+timex\n",
    "        timey=np.array(final9[i]['Encounter(%)'])+timey\n",
    "        \n",
    "    timex=timex/5\n",
    "    timey=timey/5\n",
    "    \n",
    "    output=pd.DataFrame(columns=['Time to ards_time(hours)','Encounter(%)'])\n",
    "    output['Time to ards_time(hours)']=timex\n",
    "    output['Encounter(%)']=timey\n",
    "    output.to_csv(PATH4+outputname+'_timecurve.csv',index=False)\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.set_xticks(np.arange(-48, 48, 12))\n",
    "    ax.set_yticks(np.arange(0, 100, 10))\n",
    "    plt.xlabel('Time to ards_time(hours)')\n",
    "    plt.ylabel('Encounter(%)')\n",
    "    plt.plot(timex, timey)\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    ###save averaged rocauc curve and precision recall cruve\n",
    "\n",
    "    # Compute macro-average ROC curve and ROC area\n",
    "\n",
    "    # First aggregate all false positive rates\n",
    "\n",
    "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
    "\n",
    "    # Then interpolate all ROC curves at this points\n",
    "    mean_tpr = np.zeros_like(all_fpr)\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
    "\n",
    "    # Finally average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    output=pd.DataFrame(columns=['fpr','tpr'])\n",
    "    output['fpr'] = all_fpr\n",
    "    output['tpr'] = mean_tpr\n",
    "    output.to_csv(PATH4+outputname+'_rocauc_final.csv')\n",
    "    \n",
    "    ##prc\n",
    "    all_precision = np.unique(np.concatenate([precision[i] for i in range(n_classes)]))\n",
    "\n",
    "    # interpolate all curves at this points\n",
    "    mean_recall = np.zeros_like(all_precision)\n",
    "    for i in range(n_classes):\n",
    "        mean_recall += interp(all_precision, precision[i], recall[i])\n",
    "\n",
    "    # Finally average it\n",
    "    mean_recall /= n_classes\n",
    "\n",
    "    output=pd.DataFrame(columns=['precision','recall'])\n",
    "    output['precision'] = all_precision\n",
    "    output['recall'] = mean_recall\n",
    "    output.to_csv(PATH4+outputname+'_prc_final.csv')\n",
    "    \n",
    " \n",
    "    \n",
    "    return bestparam,final1,final2,final11,final22,final3,final4,final5,final6,trainauc1,trainauc2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH1='Z:\\patient-adjudication-results\\\\'\n",
    "PATH2='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\final_datasets_alternative\\\\'\n",
    "PATH3='Z:\\project-datasets\\ARDS\\ml_algorithms\\\\'\n",
    "PATH4='Z:\\project-datasets\\ARDS\\ml_algorithms\\model_outputs_testing\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_on_intubated=True\n",
    "use_sample_weight=True\n",
    "choose_model='logistic regression'\n",
    "classweight=None #None or 'balanced'\n",
    "ifcalibrate=False\n",
    "#what data we want to use: structured, structured+unstructured, structured+unstructured+order, or structured+unstructured+clinicalnotes\n",
    "file='structured'\n",
    "if choose_model=='logistic regression':\n",
    "    MAX_EVALS = 20\n",
    "else:\n",
    "    MAX_EVALS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configure hyperopt\n",
    "global  ITERATION\n",
    "ITERATION = 0\n",
    "#Optimization Algorithm\n",
    "global tpe_algorithm\n",
    "tpe_algorithm = tpe.suggest\n",
    "\n",
    "# Trials object to track progress\n",
    "global bayes_trials\n",
    "bayes_trials = Trials()\n",
    "\n",
    "#save result history\n",
    "# File to save first results\n",
    "out_file = PATH4+'trials_'+file+'_'+choose_model+'_nested.csv'\n",
    "of_connection = open(out_file, 'w')\n",
    "writer = csv.writer(of_connection)\n",
    "\n",
    "# Write the headers to the file\n",
    "writer.writerow(['loss', 'params', 'iteration','train_time'])\n",
    "of_connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###hyperopt\n",
    "#objective function\n",
    "global trainset\n",
    "global trainset2\n",
    "trainset=0\n",
    "trainset2=0\n",
    "def objective(params):\n",
    "    # Keep track of evals\n",
    "    global ITERATION\n",
    "    global trainset\n",
    "    global trainset2\n",
    "    \n",
    "    ITERATION += 1\n",
    "    \n",
    "    \n",
    "    start = timer()\n",
    "    \n",
    "    # Perform 5_folds cross validation\n",
    "    if choose_model=='logistic regression':\n",
    "        score =crossvalidate(LogisticRegression(**params),trainset,trainset2,useweight=use_sample_weight)\n",
    "    elif choose_model=='random forest':\n",
    "        score =crossvalidate(RandomForestClassifier(**params),trainset,trainset2,useweight=use_sample_weight)\n",
    "    elif choose_model=='lda':\n",
    "        score =crossvalidate(LinearDiscriminantAnalysis(**params),trainset,trainset2,useweight=use_sample_weight)\n",
    "    elif choose_model=='lightgbm':\n",
    "        score =crossvalidate(lgb.LGBMClassifier(**params),trainset,trainset2,useweight=use_sample_weight)\n",
    "    \n",
    "    run_time = timer() - start\n",
    "    \n",
    "    \n",
    "    # Loss must be minimized\n",
    "    loss = 1-score\n",
    "    \n",
    "   \n",
    "    # Write to the csv file ('a' means append)\n",
    "    of_connection = open(out_file, 'a')\n",
    "    writer = csv.writer(of_connection)\n",
    "    writer.writerow([loss, params, ITERATION,run_time])\n",
    "    \n",
    "    # Dictionary with information for evaluation\n",
    "    return {'loss': loss, 'params': params, 'iteration': ITERATION,\n",
    "            'train_time': run_time, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "# Define the search space\n",
    "def uniform_int(name, lower, upper):\n",
    "    # `quniform` returns:\n",
    "    # round(uniform(low, high) / q) * q\n",
    "    return hp.quniform(name, lower, upper, q=1)\n",
    "\n",
    "def loguniform_int(name, lower, upper):\n",
    "    # Do not forget to make a logarithm for the\n",
    "    # lower and upper bounds.\n",
    "    return hp.qloguniform(name, np.log(lower), np.log(upper), q=1)\n",
    "\n",
    "#logistic regression\n",
    "space1={'penalty' : hp.choice('penalty', ['l1']),\n",
    " 'C':hp.uniform('C', 0.001,0.2),\n",
    " 'class_weight':hp.choice('class_weight', [classweight]),\n",
    " 'n_jobs':hp.choice('n_jobs', [-1]),\n",
    " 'random_state':hp.choice('random_state', [1234])}\n",
    "\n",
    "#random forest\n",
    "\n",
    "space2 = {'bootstrap': hp.choice('bootstrap', [True]),\n",
    "'max_depth': uniform_int('max_depth', 2, 30),\n",
    "'max_features': hp.choice('max_features', ['auto']),\n",
    "'min_samples_leaf': uniform_int('min_samples_leaf', 2, 30),\n",
    "'min_samples_split': uniform_int('min_samples_split',10, 200),\n",
    "'n_estimators': hp.choice('n_estimators', [200,400]),\n",
    "'class_weight':hp.choice('class_weight', [classweight]),\n",
    "'n_jobs':hp.choice('n_jobs', [-1]),\n",
    "'random_state':hp.choice('random_state', [1234])}\n",
    "\n",
    "#lightgbm\n",
    "\n",
    "space3 = {\n",
    "        'class_weight':hp.choice('class_weight', [classweight]),\n",
    "        'num_leaves': hp.quniform('num_leaves', 4, 32, 1),\n",
    "        'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.1)),\n",
    "        'subsample': hp.uniform('subsample', 0.2, 0.6), #alias \"subsample\"\n",
    "        'min_data_in_leaf': hp.qloguniform('min_data_in_leaf', 10, 200, 1),\n",
    "        'reg_alpha': hp.uniform('reg_alpha', 0.1, 0.6), #alias \"subsample\"\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 0.5),\n",
    "        'max_depth': uniform_int('max_depth', 2, 30),\n",
    "        'objective':'binary',\n",
    "        'silent':False,\n",
    "        'n_estimators': hp.choice('n_estimators', [200,400]),\n",
    "        'random_state':hp.choice('random_state', [1234]),\n",
    "        'n_jobs':hp.choice('n_jobs', [-1])\n",
    "    \n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "#lda\n",
    "space4={'solver':'lsqr',\n",
    "            'shrinkage': hp.uniform('shrinkage', 0, 0.9),\n",
    "            'n_components':None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ards=pd.read_csv(PATH1+'current-ards-review-results_2_25_2020.csv',dtype={'mrn': str})\n",
    "ards.rename(columns={'encounterid':'EncounterID'},inplace=True)\n",
    "ards.loc[ards['ards_time']=='.','ards_time']=np.nan\n",
    "ards.ards_time=pd.to_datetime(ards.ards_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We've settled on using data binned every 6h for training, and data binned every 2h for validation and testing\n",
    "#This script can run on any training data in the format of 'EncounterID','PatientID','ards','time','sampleweight',features\n",
    "if 'only' in file:\n",
    "    bintrain=0\n",
    "    bintest=0\n",
    "    filename1=PATH2+file+'_train.csv'\n",
    "    filename2=PATH2+file+'_train.csv'\n",
    "else:\n",
    "    filename1=PATH2+file+'_6Htrain.csv'\n",
    "    filename2=PATH2+file+'_2Htrain.csv'\n",
    "    bintrain=int(filename1.split('_')[-1][0])\n",
    "    bintest=int(filename2.split('_')[-1][0])\n",
    "    \n",
    "train_str=pd.read_csv(filename1)\n",
    "train_str2=pd.read_csv(filename2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get list of encounters that have been intubated \n",
    "structured=pd.read_csv(PATH2+'structured_data.csv')\n",
    "trainset=ards[(ards['year']==2016)&(ards['not_reviewed']==0)&(ards['not_cohort']==0)&(pd.notnull(ards['pt_ards']))].EncounterID.unique().tolist()\n",
    "intubated=structured[structured['EncounterID'].isin(trainset)]\n",
    "intubated=intubated[intubated['support']=='invasive']\n",
    "intubated_encounters=intubated.EncounterID.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestparam,scores1,scores2,scores11,scores22,scores3,scores4,scores5,scores6,trainauc1,trainauc2=nested(choose_model,train_str,train_str,same=True,outputname=file+' '+choose_model+'_nested',isintubated=test_on_intubated,useweight=use_sample_weight,calibrate=ifcalibrate,inputintubated=train_str[train_str['EncounterID'].isin(intubated_encounters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bestparam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('bin rocauc:',np.mean(scores1,axis=0),'enc rocauc:',np.mean(scores2,axis=0),'bin prcauc',np.mean(scores11,axis=0),'enc prcauc',np.mean(scores22,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train bin rocauc:',np.mean(trainauc1,axis=0),'train enc rocauc:',np.mean(trainauc2,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('sensitivity:',np.mean(scores3,axis=0),'specificity:',np.mean(scores4,axis=0),'ppv:',np.mean(scores5,axis=0),'timediff:',np.mean(scores6,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###time curve\n",
    "filenames=[PATH4+file+' '+choose_model+'_nested'+'_timecurve.csv']\n",
    "n_classes=len(filenames)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(np.arange(-48, 48, 12))\n",
    "ax.set_yticks(np.arange(0, 100, 10))\n",
    "plt.xlabel('Time to ards_time(hours)')\n",
    "plt.ylabel('Encounter(%)')\n",
    "colors=['red','green','blue','orange','purple','gray']\n",
    "for i in range(n_classes):\n",
    "    temp=pd.read_csv(filenames[i])\n",
    "    plt.plot(temp['Time to ards_time(hours)'], temp['Encounter(%)'],\n",
    "             label=filenames[i].split('\\\\')[-1].split('_')[0],color=colors[i],linewidth=0.8,linestyle='-')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "plt.grid()\n",
    "plt.show()\n",
    "fig.savefig(PATH4+file+' '+choose_model+'_nested'+\"_timecurve.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc auc\n",
    "filenames=[PATH4+file+' '+choose_model+'_nested'+'_rocauc.csv']\n",
    "n_classes=len(filenames)\n",
    "fpr={}\n",
    "tpr={}\n",
    "roc_auc ={}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    temp=pd.read_csv(filenames[i])\n",
    "    fpr[i], tpr[i]=temp['fpr'],temp['tpr']\n",
    "    roc_auc[i] = metrics.auc(fpr[i], tpr[i])\n",
    "    \n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure()\n",
    "\n",
    "colors=['red','green','blue','orange','purple','gray']\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i],\n",
    "             label=filenames[i].split('\\\\')[-1].split('_')[0]+'(area = {0:0.4f})'\n",
    "                   ''.format(roc_auc[i]),\n",
    "             color=colors[i],  linewidth=0.8,linestyle='-')\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC AUC Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "fig.savefig(PATH4+file+' '+choose_model+'_nested'+\"_rocauc.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prc\n",
    "filenames=[PATH4+file+' '+choose_model+'_nested'+'_prc.csv']\n",
    "n_classes=len(filenames)\n",
    "precision={}\n",
    "recall={}\n",
    "prc ={}\n",
    "\n",
    "for i in range(n_classes):\n",
    "    temp=pd.read_csv(filenames[i])\n",
    "    precision[i], recall[i]=temp['precision'],temp['recall']\n",
    "    prc[i] = metrics.auc( recall[i],precision[i])\n",
    "    \n",
    "\n",
    "# Plot all ROC curves\n",
    "fig=plt.figure()\n",
    "\n",
    "colors=['red','green','blue','orange','purple','gray']\n",
    "for i in range(n_classes):\n",
    "    plt.plot(recall[i],precision[i], \n",
    "             label=filenames[i].split('\\\\')[-1].split('_')[0]+'(area = {0:0.4f})'\n",
    "                   ''.format(prc[i]),\n",
    "             color=colors[i],  linewidth=0.8,linestyle='-')\n",
    "\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Recall Curve')\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()\n",
    "fig.savefig(PATH4+file+' '+choose_model+'_nested'+\"_prc.pdf\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
